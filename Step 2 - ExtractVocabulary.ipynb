{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"./data/mlm_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['^^^what is amazon ec2?^^^ amazon elastic compute cloud (amazon ec2) provides on-demand, scalable computing capacity in the amazon web services (aws) cloud. using amazon ec2 reduces hardware costs so you can develop and deploy applications faster. you can use amazon ec2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. you can add capacity (scale up) to handle compute-heavy tasks, such as monthly or yearly processes, or spikes in website traffic. when usage decreases, you can reduce capacity (scale down) again. the following diagram shows a basic architecture of an amazon ec2 instance deployed within an amazon virtual private cloud (vpc) in this example, the ec2 instance is within an availability zone in the region. the ec2 instance is secured with a security group, which is a virtual firewall that controls incoming and outgoing traffic.',\n",
       " 'a private key is stored on the local computer and a public key is stored on the instance. both keys are specified as a key pair to prove the identity of the user. in this scenario, the instance is backed by an amazon ebs volume. the vpc communicates with the internet using an internet gateway. for more information about amazon vpc, see the amazon vpc user guide. this user guide provides information specific to running linux-based instances on amazon ec2 see the ec2 user guide for windows instances for information to help you run windows-based instances on ec2 amazon ec2 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with payment card industry (pci) data security standard (dss) for more information about pci dss, including how to request a copy of the aws pci compliance package, see pci dss level 1',\n",
       " 'if you are looking for technical guidance about amazon ec2, try aws re:post. for more information about cloud computing, see what is cloud computing? ^^features of amazon ec2^^ amazon ec2 provides the following high-level features: virtual servers. preconfigured templates for your instances that package the components you need for your server (including the operating system and additional software) various configurations of cpu, memory, storage, networking capacity, and graphics hardware for your instances. secure login information for your instances. aws stores the public key and you store the private key in a secure place. storage volumes for temporary data that is deleted when you stop, hibernate, or terminate your instance. persistent storage volumes for your data using amazon elastic block store (amazon ebs) multiple physical locations for your resources, such as instances and amazon ebs volumes.',\n",
       " 'a virtual firewall that allows you to specify the protocols, ports, and source ip ranges that can reach your instances, and the destination ip ranges to which your instances can connect. static ipv4 addresses for dynamic cloud computing. metadata that you can create and assign to your amazon ec2 resources. virtual networks you can create that are logically isolated from the rest of the aws cloud. you can optionally connect these virtual networks to your own network. for details about all of the features of amazon ec2, see amazon ec2 features. for options to run your website on aws, see web hosting. ^^get started with amazon ec2^^ the following topics can help you get started with amazon ec2 after you set up to use ec2, you can walk through tutorial: get started with amazon ec2 linux instances to launch, connect to, and clean up an instance. the remaining topics point to more information about the high-level features of ec2 set up to use amazon ec2',\n",
       " 'tutorial: get started with amazon ec2 linux instances. connect to your linux instance. transfer files. instances and amis. regions and zones. instance types. tags. key pairs. security groups. elastic ip addresses. virtual private clouds. amazon ebs. instance store. remotely run commands on an ec2 instance with aws systems manager. install lamp on amazon linux 2023 configure ssl/tls on amazon linux 2023 host a wordpress blog. troubleshoot ec2 instances. aws re:post. ^^related services^^ you can provision amazon ec2 resources, such as instances and volumes, directly using amazon ec2 in addition, you can provision ec2 resources using other aws services, such as the following: amazon ec2 auto scaling. helps ensure you have the correct number of amazon ec2 instances available to handle the load for your application. aws cloudformation. helps you model and set up your aws resources using templates. aws elastic beanstalk.',\n",
       " 'deploy and manage applications in the aws cloud without having to understand the underlying infrastructure. aws opsworks. automate how servers are configured, deployed, and managed across your amazon ec2 instances using chef and puppet. ec2 image builder. automate the creation, management, and deployment of customized, secure, and up-to-date server images. aws launch wizard. size, configure, and deploy aws resources for third-party applications without having to manually identify and provision individual aws resources. amazon lightsail. to build websites or web applications, you can deploy and manage basic cloud resources using amazon lightsail. to compare the features of amazon ec2 and lightsail for your use case, see amazon lightsail or amazon ec2 elastic load balancing. automatically distribute incoming application traffic across multiple instances. amazon relational database service (amazon rds) set up, operate, and scale a managed relational database in the cloud.',\n",
       " \"although you can set up a database on an ec2 instance, amazon rds offers the advantage of handling your database management tasks, such as patching the software, backing up, and storing the backups. amazon elastic container service (amazon ecs) deploy, manage, and scale containerized applications on a cluster of ec2 instances. amazon cloudwatch. monitor your instances and amazon ebs volumes. amazon guardduty. detect potentially unauthorized or malicious use of your ec2 instances. aws backup. automate backing up your amazon ec2 instances and the amazon ebs volumes attached to them. ^^access amazon ec2^^ you can create and manage your amazon ec2 instances using the following interfaces: a simple web interface to create and manage amazon ec2 instances and resources. if you've signed up for an aws account, you can access the amazon ec2 console by signing into the aws management console and selecting ec2 from the console home page.\",\n",
       " 'enables you to interact with aws services using commands in your command-line shell. it is supported on windows, mac, and linux. for more information about the aws cli , see aws command line interface user guide. you can find the amazon ec2 commands in the aws cli command reference. a set of powershell modules that are built on the functionality exposed by the aws sdk for .net. the tools for powershell enable you to script operations on your aws resources from the powershell command line. to get started, see the aws tools for windows powershell user guide. you can find the cmdlets for amazon ec2, in the aws tools for powershell cmdlet reference. amazon ec2 supports creating resources using aws cloudformation. you create a template, in json or yaml format, that describes your aws resources, and aws cloudformation provisions and configures those resources for you.',\n",
       " 'you can reuse your cloudformation templates to provision the same resources multiple times, whether in the same region and account or in multiple regions and accounts. for more information about supported resource types and properties for amazon ec2, see ec2 resource type reference in the aws cloudformation user guide. amazon ec2 provides a query api. these requests are http or https requests that use the http verbs get or post and a query parameter named action. for more information about the api actions for amazon ec2, see actions in the amazon ec2 api reference. if you prefer to build applications using language-specific apis instead of submitting a request over http or https, aws provides libraries, sample code, tutorials, and other resources for software developers. these libraries provide basic functions that automate tasks such as cryptographically signing your requests, retrying requests, and handling error responses, making it easier for you to get started. for more information, see tools to build on aws. ^^pricing for amazon ec2^^',\n",
       " 'amazon ec2 provides the following pricing options: you can get started with amazon ec2 for free. to explore the free tier options, see aws free tier. pay for the instances that you use by the second, with a minimum of 60 seconds, with no long-term commitments or upfront payments. you can reduce your amazon ec2 costs by making a commitment to a consistent amount of usage, in usd per hour, for a term of 1 or 3 years. you can reduce your amazon ec2 costs by making a commitment to a specific instance configuration, including instance type and region, for a term of 1 or 3 years. request unused ec2 instances, which can reduce your amazon ec2 costs significantly. reduce costs by using a physical ec2 server that is fully dedicated for your use, either on-demand or as part of a savings plan. you can use your existing server-bound software licenses and get help meeting compliance requirements.',\n",
       " 'reserve compute capacity for your ec2 instances in a specific availability zone for any duration of time. removes the cost of unused minutes and seconds from your bill. for a complete list of charges and prices for amazon ec2 and more information about the purchase models, see amazon ec2 pricing. ^estimates, billing, and cost optimization^ to create estimates for your aws use cases, use the aws pricing calculator. to see your bill, go to the billing and cost management dashboard in the aws billing and cost management console. your bill contains links to usage reports that provide details about your bill. to learn more about aws account billing, see aws billing and cost management user guide. if you have questions concerning aws billing, accounts, and events, contact aws support. to calculate the cost of a sample provisioned environment, see cloud economics center. when calculating the cost of a provisioned environment, remember to include incidental costs such as snapshot storage for ebs volumes.you can optimize the cost, security, and performance of your aws environment using aws trusted advisor.',\n",
       " '^^^what is amazon ec2?^^^ amazon elastic compute cloud (amazon ec2) provides on-demand, scalable computing capacity in the amazon web services (aws) cloud. using amazon ec2 reduces hardware costs so you can develop and deploy applications faster. you can use amazon ec2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. you can add capacity (scale up) to handle compute-heavy tasks, such as monthly or yearly processes, or spikes in website traffic. when usage decreases, you can reduce capacity (scale down) again. the following diagram shows a basic architecture of an amazon ec2 instance deployed within an amazon virtual private cloud (vpc) in this example, the ec2 instance is within an availability zone in the region. the ec2 instance is secured with a security group, which is a virtual firewall that controls incoming and outgoing traffic.',\n",
       " 'a private key is stored on the local computer and a public key is stored on the instance. both keys are specified as a key pair to prove the identity of the user. in this scenario, the instance is backed by an amazon ebs volume. the vpc communicates with the internet using an internet gateway. for more information about amazon vpc, see the amazon vpc user guide. this user guide provides information specific to running windows-based instances on amazon ec2 see the ec2 user guide for linux instances for information to help you run linux-based instances on ec2 amazon ec2 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with payment card industry (pci) data security standard (dss) for more information about pci dss, including how to request a copy of the aws pci compliance package, see pci dss level 1',\n",
       " 'if you are looking for technical guidance about amazon ec2, try aws re:post. for more information about cloud computing, see what is cloud computing? ^^features of amazon ec2^^ amazon ec2 provides the following high-level features: virtual servers. preconfigured templates for your instances that package the components you need for your server (including the operating system and additional software) various configurations of cpu, memory, storage, networking capacity, and graphics hardware for your instances. secure login information for your instances. aws stores the public key and you store the private key in a secure place. storage volumes for temporary data that is deleted when you stop, hibernate, or terminate your instance. persistent storage volumes for your data using amazon elastic block store (amazon ebs) multiple physical locations for your resources, such as instances and amazon ebs volumes.',\n",
       " 'a virtual firewall that allows you to specify the protocols, ports, and source ip ranges that can reach your instances, and the destination ip ranges to which your instances can connect. static ipv4 addresses for dynamic cloud computing. metadata that you can create and assign to your amazon ec2 resources. virtual networks you can create that are logically isolated from the rest of the aws cloud. you can optionally connect these virtual networks to your own network. for details about all of the features of amazon ec2, see amazon ec2 features. windows-specific features and use case information can be found at windows server on aws. for options to run your website on aws, see web hosting. ^^get started with amazon ec2^^ the following topics can help you get started with amazon ec2 after you set up to use ec2, you can walk through tutorial: get started with amazon ec2 windows instances to launch, connect to, and clean up an instance.',\n",
       " 'the remaining topics point to more information about the high-level features of ec2 set up to use amazon ec2 tutorial: get started with amazon ec2 windows instances. connect to your windows instance. transfer files to windows instances. amazon ec2 windows instances. instance types. tags. key pairs. security groups. elastic ip addresses. virtual private clouds. amazon ebs. instance store. aws systems manager run command in the aws systems manager user guide. tutorial: get started with amazon ec2 windows instances. troubleshoot ec2 windows instances. aws re:post. ^^related services^^ you can provision amazon ec2 resources, such as instances and volumes, directly using amazon ec2 in addition, you can provision ec2 resources using other aws services, such as the following: amazon ec2 auto scaling. helps ensure you have the correct number of amazon ec2 instances available to handle the load for your application. aws cloudformation. helps you model and set up your aws resources using templates.',\n",
       " 'aws elastic beanstalk. deploy and manage applications in the aws cloud without having to understand the underlying infrastructure. aws opsworks. automate how servers are configured, deployed, and managed across your amazon ec2 instances using chef and puppet. ec2 image builder. automate the creation, management, and deployment of customized, secure, and up-to-date server images. aws launch wizard. size, configure, and deploy aws resources for third-party applications without having to manually identify and provision individual aws resources. amazon lightsail. to build websites or web applications, you can deploy and manage basic cloud resources using amazon lightsail. to compare the features of amazon ec2 and lightsail for your use case, see amazon lightsail or amazon ec2 elastic load balancing. automatically distribute incoming application traffic across multiple instances. amazon relational database service (amazon rds) set up, operate, and scale a managed relational database in the cloud.',\n",
       " \"although you can set up a database on an ec2 instance, amazon rds offers the advantage of handling your database management tasks, such as patching the software, backing up, and storing the backups. amazon elastic container service (amazon ecs) deploy, manage, and scale containerized applications on a cluster of ec2 instances. amazon cloudwatch. monitor your instances and amazon ebs volumes. amazon guardduty. detect potentially unauthorized or malicious use of your ec2 instances. aws backup. automate backing up your amazon ec2 instances and the amazon ebs volumes attached to them. ^^access amazon ec2^^ you can create and manage your amazon ec2 instances using the following interfaces: a simple web interface to create and manage amazon ec2 instances and resources. if you've signed up for an aws account, you can access the amazon ec2 console by signing into the aws management console and selecting ec2 from the console home page.\",\n",
       " 'enables you to interact with aws services using commands in your command-line shell. it is supported on windows, mac, and linux. for more information about the aws cli , see aws command line interface user guide. you can find the amazon ec2 commands in the aws cli command reference. a set of powershell modules that are built on the functionality exposed by the aws sdk for .net. the tools for powershell enable you to script operations on your aws resources from the powershell command line. to get started, see the aws tools for windows powershell user guide. you can find the cmdlets for amazon ec2, in the aws tools for powershell cmdlet reference. amazon ec2 supports creating resources using aws cloudformation. you create a template, in json or yaml format, that describes your aws resources, and aws cloudformation provisions and configures those resources for you.',\n",
       " 'you can reuse your cloudformation templates to provision the same resources multiple times, whether in the same region and account or in multiple regions and accounts. for more information about supported resource types and properties for amazon ec2, see ec2 resource type reference in the aws cloudformation user guide. amazon ec2 provides a query api. these requests are http or https requests that use the http verbs get or post and a query parameter named action. for more information about the api actions for amazon ec2, see actions in the amazon ec2 api reference. if you prefer to build applications using language-specific apis instead of submitting a request over http or https, aws provides libraries, sample code, tutorials, and other resources for software developers. these libraries provide basic functions that automate tasks such as cryptographically signing your requests, retrying requests, and handling error responses, making it easier for you to get started. for more information, see tools to build on aws. ^^pricing for amazon ec2^^',\n",
       " 'amazon ec2 provides the following pricing options: you can get started with amazon ec2 for free. to explore the free tier options, see aws free tier. pay for the instances that you use by the second, with a minimum of 60 seconds, with no long-term commitments or upfront payments. you can reduce your amazon ec2 costs by making a commitment to a consistent amount of usage, in usd per hour, for a term of 1 or 3 years. you can reduce your amazon ec2 costs by making a commitment to a specific instance configuration, including instance type and region, for a term of 1 or 3 years. request unused ec2 instances, which can reduce your amazon ec2 costs significantly. reduce costs by using a physical ec2 server that is fully dedicated for your use, either on-demand or as part of a savings plan. you can use your existing server-bound software licenses and get help meeting compliance requirements.',\n",
       " 'reserve compute capacity for your ec2 instances in a specific availability zone for any duration of time. removes the cost of unused minutes and seconds from your bill. for a complete list of charges and prices for amazon ec2 and more information about the purchase models, see amazon ec2 pricing. ^estimates, billing, and cost optimization^ to create estimates for your aws use cases, use the aws pricing calculator. to estimate the cost of transforming microsoft workloads to a modern architecture that uses open source and cloud-native services deployed on aws, use the aws modernization calculator for microsoft workloads. to see your bill, go to the billing and cost management dashboard in the aws billing and cost management console. your bill contains links to usage reports that provide details about your bill. to learn more about aws account billing, see aws billing and cost management user guide. if you have questions concerning aws billing, accounts, and events, contact aws support.to calculate the cost of a sample provisioned environment, see cloud economics center. when calculating the cost of a provisioned environment, remember to include incidental costs such as snapshot storage for ebs volumes. you can optimize the cost, security, and performance of your aws environment using aws trusted advisor.',\n",
       " '^^^what is amazon s3?^^^ amazon simple storage service (amazon s3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. customers of all sizes and industries can use amazon s3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, iot devices, and big data analytics. amazon s3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements. ^^features of amazon s3^^ ^storage classes^ amazon s3 offers a range of storage classes designed for different use cases.',\n",
       " 'for example, you can store mission-critical production data in s3 standard for frequent access, save costs by storing infrequently accessed data in s3 standard-ia or s3 one zone-ia, and archive data at the lowest costs in s3 glacier instant retrieval, s3 glacier flexible retrieval, and s3 glacier deep archive. you can store data with changing or unknown access patterns in s3 intelligent-tiering, which optimizes storage costs by automatically moving your data between four access tiers when your access patterns change. these four access tiers include two low-latency access tiers optimized for frequent and infrequent access, and two opt-in archive access tiers designed for asynchronous access for rarely accessed data. for more information, see using amazon s3 storage classes. for more information about s3 glacier flexible retrieval, see the amazon s3 glacier developer guide. ^storage management^',\n",
       " 'amazon s3 has storage management features that you can use to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements. s3 lifecycle – configure a lifecycle configuration to manage your objects and store them cost effectively throughout their lifecycle. you can transition objects to other s3 storage classes or expire objects that reach the end of their lifetimes. s3 object lock – prevent amazon s3 objects from being deleted or overwritten for a fixed amount of time or indefinitely. you can use object lock to help meet regulatory requirements that require write-once-read-many (worm) storage or to simply add another layer of protection against object changes and deletions. s3 replication – replicate objects and their respective metadata and object tags to one or more destination buckets in the same or different aws regions for reduced latency, compliance, security, and other use cases.',\n",
       " 's3 batch operations – manage billions of objects at scale with a single s3 api request or a few clicks in the amazon s3 console. you can use batch operations to perform operations such as copy, invoke aws lambda function, and restore on millions or billions of objects. ^access management and security^ amazon s3 provides features for auditing and managing access to your buckets and objects. by default, s3 buckets and the objects in them are private. you have access only to the s3 resources that you create. to grant granular resource permissions that support your specific use case or to audit the permissions of your amazon s3 resources, you can use the following features. s3 block public access – block public access to s3 buckets and objects. by default, block public access settings are turned on at the bucket level. we recommend that you keep all block public access settings enabled unless you know that you need to turn off one or more of them for your specific use case.',\n",
       " 'for more information, see configuring block public access settings for your s3 buckets. aws identity and access management (iam) – iam is a web service that helps you securely control access to aws resources, including your amazon s3 resources. with iam, you can centrally manage permissions that control which aws resources users can access. you use iam to control who is authenticated (signed in) and authorized (has permissions) to use resources. bucket policies – use iam-based policy language to configure resource-based permissions for your s3 buckets and the objects in them. amazon s3 access points – configure named network endpoints with dedicated access policies to manage data access at scale for shared datasets in amazon s3 access control lists (acls) – grant read and write permissions for individual buckets and objects to authorized users. as a general rule, we recommend using s3 resource-based policies (bucket policies and access point policies) or iam user policies for access control instead of acls.',\n",
       " \"policies are a simplified and more flexible access control option. with bucket policies and access point policies, you can define rules that apply broadly across all requests to your amazon s3 resources. for more information about the specific cases when you'd use acls instead of resource-based policies or iam user policies, see access policy guidelines. s3 object ownership – take ownership of every object in your bucket, simplifying access management for data stored in amazon s3 s3 object ownership is an amazon s3 bucket-level setting that you can use to disable or enable acls. by default, acls are disabled. with acls disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies. iam access analyzer for s3 – evaluate and monitor your s3 bucket access policies, ensuring that the policies provide only the intended access to your s3 resources. ^data processing^\",\n",
       " 'to transform data and trigger workflows to automate a variety of other processing activities at scale, you can use the following features. s3 object lambda – add your own code to s3 get, head, and list requests to modify and process data as it is returned to an application. filter rows, dynamically resize images, redact confidential data, and much more. event notifications – trigger workflows that use amazon simple notification service (amazon sns), amazon simple queue service (amazon sqs), and aws lambda when a change is made to your s3 resources. ^storage logging and monitoring^ amazon s3 provides logging and monitoring tools that you can use to monitor and control how your amazon s3 resources are being used. for more information, see monitoring tools. amazon cloudwatch metrics for amazon s3 – track the operational health of your s3 resources and configure billing alerts when estimated charges reach a user-defined threshold.',\n",
       " 'aws cloudtrail – record actions taken by a user, a role, or an aws service in amazon s3 cloudtrail logs provide you with detailed api tracking for s3 bucket-level and object-level operations. server access logging – get detailed records for the requests that are made to a bucket. you can use server access logs for many use cases, such as conducting security and access audits, learning about your customer base, and understanding your amazon s3 bill. aws trusted advisor – evaluate your account by using aws best practice checks to identify ways to optimize your aws infrastructure, improve security and performance, reduce costs, and monitor service quotas. you can then follow the recommendations to optimize your services and resources. ^analytics and insights^ amazon s3 offers features to help you gain visibility into your storage usage, which empowers you to better understand, analyze, and optimize your storage at scale. amazon s3 storage lens – understand, analyze, and optimize your storage.',\n",
       " \"s3 storage lens provides 60+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, aws regions, buckets, or prefixes. storage class analysis – analyze storage access patterns to decide when it's time to move data to a more cost-effective storage class. s3 inventory with inventory reports – audit and report on objects and their corresponding metadata and configure other amazon s3 features to take action in inventory reports. for example, you can report on the replication and encryption status of your objects. for a list of all the metadata available for each object in inventory reports, see amazon s3 inventory list. ^strong consistency^ amazon s3 provides strong read-after-write consistency for put and delete requests of objects in your amazon s3 bucket in all aws regions. this behavior applies to both writes of new objects as well as put requests that overwrite existing objects and delete requests.\",\n",
       " 'in addition, read operations on amazon s3 select, amazon s3 access control lists (acls), amazon s3 object tags, and object metadata (for example, the head object) are strongly consistent. for more information, see amazon s3 data consistency model. ^^how amazon s3 works^^ amazon s3 is an object storage service that stores data as objects within buckets. an object is a file and any metadata that describes the file. a bucket is a container for objects. to store your data in amazon s3, you first create a bucket and specify a bucket name and aws region. then, you upload your data to that bucket as objects in amazon s3 each object has a key (or key name), which is the unique identifier for the object within the bucket. s3 provides features that you can configure to support your specific use case.',\n",
       " 'for example, you can use s3 versioning to keep multiple versions of an object in the same bucket, which allows you to restore objects that are accidentally deleted or overwritten. buckets and the objects in them are private and can be accessed only if you explicitly grant access permissions. you can use bucket policies, aws identity and access management (iam) policies, access control lists (acls), and s3 access points to manage access. ^buckets^ a bucket is a container for objects stored in amazon s3 you can store any number of objects in a bucket and can have up to 100 buckets in your account. to request an increase, visit the service quotas console. every object is contained in a bucket.',\n",
       " 'for example, if the object named photos/puppy.jpg is stored in the doc-example-bucket bucket in the us west (oregon) region, then it is addressable by using the url https://doc-example-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg. for more information, see accessing a bucket. when you create a bucket, you enter a bucket name and choose the aws region where the bucket will reside. after you create a bucket, you cannot change the name of the bucket or its region. bucket names must follow the bucket naming rules. you can also configure a bucket to use s3 versioning or other storage management features. buckets also: organize the amazon s3 namespace at the highest level. identify the account responsible for storage and data transfer charges. provide access control options, such as bucket policies, access control lists (acls), and s3 access points, that you can use to manage access to your amazon s3 resources.',\n",
       " 'serve as the unit of aggregation for usage reporting. for more information about buckets, see buckets overview. ^objects^ objects are the fundamental entities stored in amazon s3 objects consist of object data and metadata. the metadata is a set of name-value pairs that describe the object. these pairs include some default metadata, such as the date last modified, and standard http metadata, such as content-type. you can also specify custom metadata at the time that the object is stored. an object is uniquely identified within a bucket by a key (name) and a version id (if s3 versioning is enabled on the bucket) for more information about objects, see amazon s3 objects overview. ^keys^ an object key (or key name) is the unique identifier for an object within a bucket. every object in a bucket has exactly one key.',\n",
       " 'the combination of a bucket, object key, and optionally, version id (if s3 versioning is enabled for the bucket) uniquely identify each object. so you can think of amazon s3 as a basic data map between \"bucket + key + version\" and the object itself. every object in amazon s3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and optionally, a version. for example, in the url https://doc-example-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg, doc-example-bucket is the name of the bucket and photos/puppy.jpg is the key. for more information about object keys, see creating object key names. ^s3 versioning^ you can use s3 versioning to keep multiple variants of an object in the same bucket. with s3 versioning, you can preserve, retrieve, and restore every version of every object stored in your buckets.',\n",
       " 'you can easily recover from both unintended user actions and application failures. for more information, see using versioning in s3 buckets. ^version id^ when you enable s3 versioning in a bucket, amazon s3 generates a unique version id for each object added to the bucket. objects that already existed in the bucket at the time that you enable versioning have a version id of null. if you modify these (or any other) objects with other operations, such as copyobject and putobject, the new objects get a unique version id. for more information, see using versioning in s3 buckets. ^bucket policy^ a bucket policy is a resource-based aws identity and access management (iam) policy that you can use to grant access permissions to your bucket and the objects in it. only the bucket owner can associate a policy with a bucket. the permissions attached to the bucket apply to all of the objects in the bucket that are owned by the bucket owner.',\n",
       " 'bucket policies are limited to 20 kb in size. bucket policies use json-based access policy language that is standard across aws. you can use bucket policies to add or deny permissions for the objects in a bucket. bucket policies allow or deny requests based on the elements in the policy, including the requester, s3 actions, resources, and aspects or conditions of the request (for example, the ip address used to make the request) for example, you can create a bucket policy that grants cross-account permissions to upload objects to an s3 bucket while ensuring that the bucket owner has full control of the uploaded objects. for more information, see bucket policy examples. in your bucket policy, you can use wildcard characters on amazon resource names (arns) and other values to grant permissions to a subset of objects. for example, you can control access to groups of objects that begin with a common prefix or end with a given extension, such as .html. ^s3 access points^',\n",
       " 'amazon s3 access points are named network endpoints with dedicated access policies that describe how data can be accessed using that endpoint. access points are attached to buckets that you can use to perform s3 object operations, such as getobject and putobject. access points simplify managing data access at scale for shared datasets in amazon s3 each access point has its own access point policy. you can configure block public access settings for each access point. to restrict amazon s3 data access to a private network, you can also configure any access point to accept requests only from a virtual private cloud (vpc) for more information, see managing data access with amazon s3 access points. ^access control lists (acls)^ you can use acls to grant read and write permissions to authorized users for individual buckets and objects. each bucket and object has an acl attached to it as a subresource. the acl defines which aws accounts or groups are granted access and the type of access.',\n",
       " 'acls are an access control mechanism that predates iam. for more information about acls, see access control list (acl) overview. s3 object ownership is an amazon s3 bucket-level setting that you can use to both control ownership of the objects that are uploaded to your bucket and to disable or enable acls. by default, object ownership is set to the bucket owner enforced setting, and all acls are disabled. when acls are disabled, the bucket owner owns all the objects in the bucket and manages access to them exclusively by using access-management policies. a majority of modern use cases in amazon s3 no longer require the use of acls. we recommend that you keep acls disabled, except in unusual circumstances where you need to control access for each object individually. with acls disabled, you can use policies to control access to all objects in your bucket, regardless of who uploaded the objects to your bucket.',\n",
       " 'for more information, see controlling ownership of objects and disabling acls for your bucket. ^regions^ you can choose the geographical aws region where amazon s3 stores the buckets that you create. you might choose a region to optimize latency, minimize costs, or address regulatory requirements. objects stored in an aws region never leave the region unless you explicitly transfer or replicate them to another region. for example, objects stored in the europe (ireland) region never leave it. you can access amazon s3 and its features only in the aws regions that are enabled for your account. for more information about enabling a region to create and manage aws resources, see managing aws regions in the aws general reference. for a list of amazon s3 regions and endpoints, see regions and endpoints in the aws general reference. ^^amazon s3 data consistency model^^ amazon s3 provides strong read-after-write consistency for put and delete requests of objects in your amazon s3 bucket in all aws regions.',\n",
       " 'this behavior applies to both writes to new objects as well as put requests that overwrite existing objects and delete requests. in addition, read operations on amazon s3 select, amazon s3 access controls lists (acls), amazon s3 object tags, and object metadata (for example, the head object) are strongly consistent. updates to a single key are atomic. for example, if you make a put request to an existing key from one thread and perform a get request on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data. amazon s3 achieves high availability by replicating data across multiple servers within aws data centers. if a put request is successful, your data is safely stored. any read (get or list request) that is initiated following the receipt of a successful put response will return the data written by the put request. here are examples of this behavior:',\n",
       " 'a process writes a new object to amazon s3 and immediately lists keys within its bucket. the new object appears in the list. a process replaces an existing object and immediately tries to read it. amazon s3 returns the new data. a process deletes an existing object and immediately tries to read it. amazon s3 does not return any data because the object has been deleted. a process deletes an existing object and immediately lists keys within its bucket. the object does not appear in the listing. amazon s3 does not support object locking for concurrent writers. if two put requests are simultaneously made to the same key, the request with the latest timestamp wins. if this is an issue, you must build an object-locking mechanism into your application. updates are key-based. there is no way to make atomic updates across keys. for example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application. bucket configurations have an eventual consistency model.',\n",
       " 'specifically, this means that: if you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list. if you enable versioning on a bucket for the first time, it might take a short amount of time for the change to be fully propagated. we recommend that you wait for 15 minutes after enabling versioning before issuing write operations (put or delete requests) on objects in the bucket. ^concurrent applications^ this section provides examples of behavior to be expected from amazon s3 when multiple clients are writing to the same items. in this example, both w1 (write 1) and w2 (write 2) finish before the start of r1 (read 1) and r2 (read 2) because s3 is strongly consistent, r1 and r2 both return color = ruby. in the next example, w2 does not finish before the start of r1',\n",
       " 'therefore, r1 might return color = ruby or color = garnet. however, because w1 and w2 finish before the start of r2, r2 returns color = garnet. in the last example, w2 begins before w1 has received an acknowledgment. therefore, these writes are considered concurrent. amazon s3 internally uses last-writer-wins semantics to determine which write takes precedence. however, the order in which amazon s3 receives the requests and the order in which applications receive acknowledgments cannot be predicted because of various factors, such as network latency. for example, w2 might be initiated by an amazon ec2 instance in the same region, while w1 might be initiated by a host that is farther away. the best way to determine the final value is to perform a read after both writes have been acknowledged. ^^related services^^ after you load your data into amazon s3, you can use it with other aws services.',\n",
       " \"the following are the services that you might use most frequently: amazon elastic compute cloud (amazon ec2) – provides secure and scalable computing capacity in the aws cloud. using amazon ec2 eliminates your need to invest in hardware upfront, so you can develop and deploy applications faster. you can use amazon ec2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. amazon emr – helps businesses, researchers, data analysts, and developers easily and cost-effectively process vast amounts of data. amazon emr uses a hosted hadoop framework running on the web-scale infrastructure of amazon ec2 and amazon s3 aws snow family – helps customers that need to run operations in austere, non-data center environments, and in locations where there's a lack of consistent network connectivity. you can use aws snow family devices to locally and cost-effectively access the storage and compute power of the aws cloud in places where an internet connection might not be an option.\",\n",
       " \"aws transfer family – provides fully managed support for file transfers directly into and out of amazon s3 or amazon elastic file system (amazon efs) using secure shell (ssh) file transfer protocol (sftp), file transfer protocol over ssl (ftps), and file transfer protocol (ftp) ^^accessing amazon s3^^ you can work with amazon s3 in any of the following ways: ^aws management console^ the console is a web-based user interface for managing amazon s3 and aws resources. if you've signed up for an aws account, you can access the amazon s3 console by signing into the aws management console and choosing s3 from the aws management console home page. ^aws command line interface^ you can use the aws command line tools to issue commands or build scripts at your system's command line to perform aws (including s3) tasks. the aws command line interface (aws cli) provides commands for a broad set of aws services.\",\n",
       " 'the aws cli is supported on windows, macos, and linux. to get started, see the aws command line interface user guide. for more information about the commands for amazon s3, see s3api and s3control in the aws cli command reference. ^aws sdks^ aws provides sdks (software development kits) that consist of libraries and sample code for various programming languages and platforms (java, python, ruby, .net, ios, android, and so on) the aws sdks provide a convenient way to create programmatic access to s3 and aws. amazon s3 is a rest service. you can send requests to amazon s3 using the aws sdk libraries, which wrap the underlying amazon s3 rest api and simplify your programming tasks. for example, the sdks take care of tasks such as calculating signatures, cryptographically signing requests, managing errors, and retrying requests automatically.',\n",
       " 'for information about the aws sdks, including how to download and install them, see tools for aws. every interaction with amazon s3 is either authenticated or anonymous. if you are using the aws sdks, the libraries compute the signature for authentication from the keys that you provide. for more information about how to make requests to amazon s3, see making requests. ^amazon s3 rest api^ the architecture of amazon s3 is designed to be programming language-neutral, using aws-supported interfaces to store and retrieve objects. you can access s3 and aws programmatically by using the amazon s3 rest api. the rest api is an http interface to amazon s3 with the rest api, you use standard http requests to create, fetch, and delete buckets and objects. to use the rest api, you can use any toolkit that supports http. you can even use a browser to fetch objects, as long as they are anonymously readable.',\n",
       " \"the rest api uses standard http headers and status codes, so that standard browsers and toolkits work as expected. in some areas, we have added functionality to http (for example, we added headers to support access control) in these cases, we have done our best to add the new functionality in a way that matches the style of standard http usage. if you make direct rest api calls in your application, you must write the code to compute the signature and add it to the request. for more information about how to make requests to amazon s3, see making requests. soap api support over http is deprecated, but it is still available over https. newer amazon s3 features are not supported for soap. we recommend that you use either the rest api or the aws sdks. ^^paying for amazon s3^^ pricing for amazon s3 is designed so that you don't have to plan for the storage requirements of your application.\",\n",
       " 'most storage providers require you to purchase a predetermined amount of storage and network transfer capacity. in this scenario, if you exceed that capacity, your service is shut off or you are charged high overage fees. if you do not exceed that capacity, you pay as though you used it all. amazon s3 charges you only for what you actually use, with no hidden fees and no overage charges. this model gives you a variable-cost service that can grow with your business while giving you the cost advantages of the aws infrastructure. for more information, see amazon s3 pricing. when you sign up for aws, your aws account is automatically signed up for all services in aws, including amazon s3 however, you are charged only for the services that you use. if you are a new amazon s3 customer, you can get started with amazon s3 for free. for more information, see aws free tier.',\n",
       " 'to see your bill, go to the billing and cost management dashboard in the aws billing and cost management console. to learn more about aws account billing, see the aws billing user guide. if you have questions concerning aws billing and aws accounts, contact aws support. ^^pci dss compliance^^ amazon s3 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with payment card industry (pci) data security standard (dss) for more information about pci dss, including how to request a copy of the aws pci compliance package, see pci dss level 1.',\n",
       " '^^^amazon s3 rest api introduction^^^ welcome to the amazon simple storage service api reference. this guide explains the amazon simple storage service (amazon s3) application programming interface (api) it describes various api operations, related request and response structures, and error codes. the current version of the amazon s3 api is 2006-03-01 amazon s3 supports the rest api. support for soap over http is deprecated, but it is still available over https. however, new amazon s3 features will not be supported for soap. we recommend that you use either the rest api or the aws sdks. read the following about authentication and access control before going to specific api topics. requests to amazon s3 can be authenticated or anonymous. authenticated access requires credentials that aws can use to authenticate your requests. when making rest api calls directly from your code, you create a signature using valid credentials and include the signature in your request.',\n",
       " \"for information about various authentication methods and signature calculations, see authenticating requests (aws signature version 4) making rest api calls directly from your code can be cumbersome. it requires you to write the necessary code to calculate a valid signature to authenticate your requests. we recommend the following alternatives instead: use the aws sdks to send your requests (see sample code and libraries) with this option, you don't need to write code to calculate a signature for request authentication because the sdk clients authenticate your requests by using access keys that you provide. unless you have a good reason not to, you should always use the aws sdks. use the aws cli to make amazon s3 api calls. for information about setting up the aws cli and example amazon s3 commands see the following topics: set up the aws cli in the amazon simple storage service user guide. using amazon s3 with the aws command line interface in the aws command line interface user guide.\",\n",
       " \"if you'd like to make your own rest api calls instead of using one of the above alternatives, there are some things to keep in mind. the rest api uses standard http headers and status codes, so standard browsers and toolkits work as expected. in some areas, we have added functionality to http (for example, we added headers to support access control) in these cases, we have done our best to add the new functionality in a way that matches the style of standard http usage. for more information about making requests, see making requests in the amazon simple storage service user guide. for additional details about developing using rest apis, see developing with amazon s3 using the rest api in the amazon simple storage service user guide. you can have valid credentials to authenticate your requests, but unless you have permissions you cannot create or access amazon s3 resources. for example, you must have permissions to create an s3 bucket or get an object from your bucket.\",\n",
       " 'if you use the root user credentials of your aws account, you have all the permissions. however, using root user credentials is not recommended. instead, we recommend that you create iam roles in your account and manage user permissions. for more information, see managing access permissions to your amazon s3 resources in the amazon simple storage service user guide.',\n",
       " \"^^^what is amazon dynamodb?^^^ amazon dynamodb is a fully managed nosql database service that provides fast and predictable performance with seamless scalability. dynamodb lets you offload the administrative burdens of operating and scaling a distributed database so that you don't have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. dynamodb also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data. for more information, see dynamodb encryption at rest. with dynamodb, you can create database tables that can store and retrieve any amount of data and serve any level of request traffic. you can scale up or scale down your tables' throughput capacity without downtime or performance degradation. you can use the aws management console to monitor resource utilization and performance metrics. dynamodb provides on-demand backup capability. it allows you to create full backups of your tables for long-term retention and archival for regulatory compliance needs.\",\n",
       " 'for more information, see using on-demand backup and restore for dynamodb. you can create on-demand backups and enable point-in-time recovery for your amazon dynamodb tables. point-in-time recovery helps protect your tables from accidental write or delete operations. with point-in-time recovery, you can restore a table to any point in time during the last 35 days. for more information, see point-in-time recovery: how it works. dynamodb allows you to delete expired items from tables automatically to help you reduce storage usage and the cost of storing data that is no longer relevant. for more information, see expiring items by using dynamodb time to live (ttl) ^^high availability and durability^^ dynamodb automatically spreads the data and traffic for your tables over a sufficient number of servers to handle your throughput and storage requirements, while maintaining consistent and fast performance.',\n",
       " 'all of your data is stored on solid-state disks (ssds) and is automatically replicated across multiple availability zones in an aws region, providing built-in high availability and data durability. you can use global tables to keep dynamodb tables in sync across aws regions. for more information, see global tables - multi-region replication for dynamodb. ^^getting started with dynamodb^^ we recommend that you begin by reading the following sections: amazon dynamodb: how it works—to learn essential dynamodb concepts. setting up dynamodb —to learn how to set up dynamodb (the downloadable version or the web service) accessing dynamodb—to learn how to access dynamodb using the console, aws cli, or api. from there, you have two options to quickly get started quickly with dynamodb: getting started with dynamodb. getting started with dynamodb and the aws sdks. to learn more about application development, see the following: programming with dynamodb and the aws sdks.',\n",
       " 'working with tables, items, queries, scans, and indexes. to quickly find recommendations for maximizing performance and minimizing throughput costs, see the following: best practices for designing and architecting with dynamodb. to learn how to tag dynamodb resources, see adding tags and labels to resources. for best practices, how-to guides, and tools, see amazon dynamodb resources. you can use aws database migration service (aws dms) to migrate data from a relational database or mongodb to a dynamodb table. for more information, see the aws database migration service user guide. to learn how to use mongodb as a migration source, see using mongodb as a source for aws database migration service. to learn how to use dynamodb as a migration target, see using an amazon dynamodb database as a target for aws database migration service. ^^dynamodb tutorials^^ the following tutorials present complete end-to-end procedures to familiarize yourself with dynamodb.these tutorials can be completed with the free tier of aws and will give you practical experience using dynamodb. build an application using a nosql key-value data store. create and query a nosql table with amazon dynamodb.',\n",
       " '@@@^^^welcome^^^@@@ amazon dynamodb provides low-level api actions for managing database tables and indexes, and for creating, reading, updating and deleting data. dynamodb also provides api actions for accessing and processing stream records. this api reference describes the low-level api for amazon dynamodb. instead of making requests to the low-level api directly from your application, we recommend that you use one of the aws software development kits (sdks) for your programming language. the aws sdks take care of request authentication, serialization, and connection management. for more information, see overview of aws sdk support for dynamodb in the amazon dynamodb developer guide. at the end of each api action description there are links to the equivalent cli command and programming-specific language method. similarly, at the end of each api datatype description, there are links to the equivalent programming-specific language type.',\n",
       " '^^^cheat sheet for dynamodb^^^ this cheat sheet provides a quick reference for working with amazon dynamodb and its various aws sdks. ^^initial setup^^ sign up for aws. get an aws access key to access dynamodb programmatically. configure your dynamodb credentials. see also: setting up dynamodb (web service) getting started with dynamodb. basic overview of core components. ^^sdk or cli^^ choose your preferred sdk, or set up the aws cli. when you use the aws cli on windows, a backslash (\\\\) that is not inside a quote is treated as a carriage return. also, you must escape any quotes and braces inside other quotes. for an example, see the windows tab in \"create a table\" in the next section. see also: aws cli with dynamodb. getting started with dynamodb - step 2 ^^basic actions^^ this section provides code for basic dynamodb tasks.',\n",
       " 'for more information about these tasks, see getting started with dynamodb and the aws sdks. ^create a table^ ~~awsdynamodbcreate-table\\\\--table-namemusic\\\\--attribute-definitions\\\\attributename=artist,attributetype=s\\\\attributename=songtitle,attributetype=s\\\\--key-schema\\\\attributename=artist,keytype=hash\\\\attributename=songtitle,keytype=range\\\\--provisioned-throughput\\\\readcapacityunits=10,writecapacityunits=5~~ ^write item to a table^ ~~awsdynamodbput-item\\\\--table-namemusic\\\\--itemfile://item.json~~ ^read item from a table^ ~~awsdynamodbget-item\\\\--table-namemusic\\\\--itemfile://item.json~~ ^delete item from a table^ ~~awsdynamodbdelete-item--table-namemusic--keyfile://key.json~~ ^query a table^',\n",
       " '~~awsdynamodbquery--table-namemusic--key-condition-expression\"artistname=:artistandsongname=:songtitle\"~~ ^delete a table^ ~~awsdynamodbdelete-table--table-namemusic~~ ^list table names^ ~~awsdynamodblist-tables~~ ^^naming rules^^ all names must be encoded using utf-8 and are case sensitive. table names and index names must be between 3 and 255 characters long, and can contain only the following characters: a-z. a-z. 0-9 _(underscore) -(dash) .(dot) attribute names must be at least one character long, and less than 64 kb in size. for more information, see naming rules. ^^service quota basics^^ read and write units. read capacity unit (rcu) – one strongly consistent read per second, or two eventually consistent reads per second, for items up to 4 kb in size.',\n",
       " 'write capacity unit (wcu) – one write per second, for items up to 1 kb in size. table limits. table size – there is no practical limit on table size. tables are unconstrained in terms of the number of items or the number of bytes. number of tables – for any aws account, there is an initial quota of 2,500 tables per aws region. page size limit for query and scan – there is a limit of 1 mb per page, per query or scan. if your query parameters or scan operation on a table result in more than 1 mb of data, dynamodb returns the initial matching items. it also returns a lastevaluatedkey property that you can use in a new request to read the next page. indexes. local secondary indexes (lsis) – you can define a maximum of five local secondary indexes. lsis are primarily useful when an index must have strong consistency with the base table.',\n",
       " \"global secondary indexes (gsis) – there is a default quota of 20 global secondary indexes per table. projected secondary index attributes per table – you can project a total of up to 100 attributes into all of a table's local and global secondary indexes. this only applies to user-specified projected attributes. partition keys. the minimum length of a partition key value is 1 byte. the maximum length is 2048 bytes. there is no practical limit on the number of distinct partition key values, for tables or for secondary indexes. the minimum length of a sort key value is 1 byte. the maximum length is 1024 bytes. in general, there is no practical limit on the number of distinct sort key values per partition key value. the exception is for tables with secondary indexes. for more information on secondary indexes, partition key design, and sort key design, see best practices. limits for commonly used data types.\",\n",
       " 'string – the length of a string is constrained by the maximum item size of 400 kb. strings are unicode with utf-8 binary encoding. number – a number can have up to 38 digits of precision, and can be positive, negative, or zero. binary – the length of a binary is constrained by the maximum item size of 400 kb. applications that work with binary attributes must encode the data in base64 encoding before sending it to dynamodb. for a full list of supported data types, see data types. for more information, also see service quotas. ^items, attributes, and expression parameters^ the maximum item size in dynamodb is 400 kb, which includes both attribute name binary length (utf-8 length) and attribute value binary lengths (utf-8 length) the attribute name counts towards the size limit.',\n",
       " 'there is no limit on the number of values in a list, map, or set, as long as the item that contains the values fits within the 400-kb item size limit. for expression parameters, the maximum length of any expression string is 4 kb. for more information about item size, attributes, and expression parameters, see service quotas. ^^more information^^ security. monitoring and logging. working with streams. backups and point-in-time recovery. integrating with other aws services. api reference. architecture center: database best practices. video tutorials. dynamodb forum.',\n",
       " '^^^what is amazon relational database service (amazon rds)?^^^ amazon relational database service (amazon rds) is a web service that makes it easier to set up, operate, and scale a relational database in the aws cloud. it provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks. this guide covers amazon rds database engines other than amazon aurora. for information about using amazon aurora, see the amazon aurora user guide. if you are new to aws products and services, begin learning more with the following resources: for an overview of all aws products, see what is cloud computing? amazon web services provides a number of database services. to learn more about the variety of database options available on aws, see choosing an aws database service and running databases on aws. ^^overview of amazon rds^^',\n",
       " 'why do you want to run a relational database in the aws cloud? because aws takes over many of the difficult and tedious management tasks of a relational database. ^amazon ec2 and on-premises databases^ amazon elastic compute cloud (amazon ec2) provides scalable computing capacity in the aws cloud. amazon ec2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. when you buy an on-premises server, you get cpu, memory, storage, and iops, all bundled together. with amazon ec2, these are split apart so that you can scale them independently. if you need more cpu, less iops, or more storage, you can easily allocate them. for a relational database in an on-premises server, you assume full responsibility for the server, operating system, and software. for a database on an amazon ec2 instance, aws manages the layers below the operating system.',\n",
       " \"in this way, amazon ec2 eliminates some of the burden of managing an on-premises database server. in the following table, you can find a comparison of the management models for on-premises databases and amazon ec2 feature. on-premises management. amazon ec2 management. application optimization. customer. customer. scaling. customer. customer. high availability. customer. customer. database backups. customer. customer. database software patching. customer. customer. database software install. customer. customer. amazon ec2 isn't a fully managed service. thus, when you run a database on amazon ec2, you're more prone to user errors. for example, when you update the operating system or database software manually, you might accidentally cause application downtime. you might spend hours checking every change to identify and fix an issue. operating system (os) patching. customer. customer. os installation. customer. customer. server maintenance. customer. aws.\",\n",
       " \"^amazon rds and amazon ec2^ hardware lifecycle. customer. aws. amazon rds is a managed database service. it's responsible for most management tasks. by eliminating tedious manual tasks, amazon rds frees you to focus on your application and your users. we recommend amazon rds over amazon ec2 as your default choice for most database deployments. power, network, and cooling. customer. aws. in the following table, you can find a comparison of the management models in amazon ec2 and amazon rds. feature. amazon ec2 management. amazon rds management. application optimization. customer. customer. scaling. customer. aws. high availability. customer. aws. database backups. customer. aws. database software patching. customer. aws. database software install. customer. aws. amazon rds provides the following specific advantages over database deployments that aren't fully managed: os patching. customer. aws. os installation. customer. aws.\",\n",
       " 'you can use the database products you are already familiar with: mariadb, microsoft sql server, mysql, oracle, and postgresql. server maintenance. aws. aws. amazon rds manages backups, software patching, automatic failure detection, and recovery. hardware lifecycle. aws. aws. you can turn on automated backups, or manually create your own backup snapshots. you can use these backups to restore a database. the amazon rds restore process works reliably and efficiently. power, network, and cooling. aws. aws. you can get high availability with a primary instance and a synchronous secondary instance that you can fail over to when problems occur. you can also use read replicas to increase read scaling. in addition to the security in your database package, you can help control who can access your rds databases. to do so, you can use aws identity and access management (iam) to define users and permissions.',\n",
       " 'you can also help protect your databases by putting them in a virtual private cloud (vpc) ^amazon rds custom for oracle and microsoft sql server^ amazon rds custom is an rds management type that gives you full access to your database and operating system. you can use the control capabilities of rds custom to access and customize the database environment and operating system for legacy and packaged business applications. meanwhile, amazon rds automates database administration tasks and operations. in this deployment model, you can install applications and change configuration settings to suit your applications. at the same time, you can offload database administration tasks such as provisioning, scaling, upgrading, and backup to aws. you can take advantage of the database management benefits of amazon rds, with more control and flexibility. for oracle database and microsoft sql server, rds custom combines the automation of amazon rds with the flexibility of amazon ec2 for more information on rds custom, see working with amazon rds custom.',\n",
       " 'with the shared responsibility model of rds custom, you get more control than in amazon rds, but also more responsibility. for more information, see shared responsibility model in rds custom. ^amazon rds on aws outposts^ amazon rds on aws outposts extends rds for sql server, rds for mysql, and rds for postgresql databases to aws outposts environments. aws outposts uses the same hardware as in public aws regions to bring aws services, infrastructure, and operation models on-premises. with rds on outposts, you can provision managed db instances close to the business applications that must run on-premises. for more information, see working with amazon rds on aws outposts. ^^db instances^^ a db instance is an isolated database environment in the aws cloud. the basic building block of amazon rds is the db instance. your db instance can contain one or more user-created databases.',\n",
       " 'you can access your db instance by using the same tools and applications that you use with a standalone database instance. you can create and modify a db instance by using the aws command line interface (aws cli), the amazon rds api, or the aws management console. ^db engines^ a db engine is the specific relational database software that runs on your db instance. amazon rds currently supports the following engines: mariadb. microsoft sql server. mysql. oracle. postgresql. each db engine has its own supported features, and each version of a db engine can include specific features. support for amazon rds features varies across aws regions and specific versions of each db engine. to check feature support in different engine versions and regions, see supported features in amazon rds by aws region and db engine. additionally, each db engine has a set of parameters in a db parameter group that control the behavior of the databases that it manages. ^db instance classes^',\n",
       " 'a db instance class determines the computation and memory capacity of a db instance. a db instance class consists of both the db instance type and the size. each instance type offers different compute, memory, and storage capabilities. for example, db.m6g is a general-purpose db instance type powered by aws graviton2 processors. within the db.m6g instance type, db.m6g.2xlarge is a db instance class. you can select the db instance that best meets your needs. if your needs change over time, you can change db instances. for information, see db instance classes. for pricing information on db instance classes, see the pricing section of the amazon rds product page. ^db instance storage^ amazon ebs provides durable, block-level storage volumes that you can attach to a running instance. db instance storage comes in the following types: general purpose (ssd) provisioned iops (piops) magnetic.',\n",
       " \"the storage types differ in performance characteristics and price. you can tailor your storage performance and cost to the needs of your database. each db instance has minimum and maximum storage requirements depending on the storage type and the database engine it supports. it's important to have sufficient storage so that your databases have room to grow. also, sufficient storage makes sure that features for the db engine have room to write content or log entries. for more information, see amazon rds db instance storage. ^amazon virtual private cloud (amazon vpc)^ you can run a db instance on a virtual private cloud (vpc) using the amazon virtual private cloud (amazon vpc) service. when you use a vpc, you have control over your virtual networking environment. you can choose your own ip address range, create subnets, and configure routing and access control lists. the basic functionality of amazon rds is the same whether it's running in a vpc or not.\",\n",
       " \"amazon rds manages backups, software patching, automatic failure detection, and recovery. there's no additional cost to run your db instance in a vpc. for more information on using amazon vpc with rds, see amazon vpc vpcs and amazon rds. amazon rds uses network time protocol (ntp) to synchronize the time on db instances. ^^aws regions and availability zones^^ amazon cloud computing resources are housed in highly available data center facilities in different areas of the world (for example, north america, europe, or asia) each data center location is called an aws region. each aws region contains multiple distinct locations called availability zones, or azs. each availability zone is engineered to be isolated from failures in other availability zones. each is engineered to provide inexpensive, low-latency network connectivity to other availability zones in the same aws region. by launching instances in separate availability zones, you can protect your applications from the failure of a single location.\",\n",
       " 'for more information, see regions, availability zones, and local zones. you can run your db instance in several availability zones, an option called a multi-az deployment. when you choose this option, amazon automatically provisions and maintains one or more secondary standby db instances in a different availability zone. your primary db instance is replicated across availability zones to each secondary db instance. this approach helps provide data redundancy and failover support, eliminate i/o freezes, and minimize latency spikes during system backups. in a multi-az db clusters deployment, the secondary db instances can also serve read traffic. for more information, see configuring and managing a multi-az deployment. ^^security^^ a security group controls the access to a db instance. it does so by allowing access to ip address ranges or amazon ec2 instances that you specify. for more information about security groups, see security in amazon rds. ^^amazon rds monitoring^^',\n",
       " 'there are several ways that you can track the performance and health of a db instance. you can use the amazon cloudwatch service to monitor the performance and health of a db instance. cloudwatch performance charts are shown in the amazon rds console. you can also subscribe to amazon rds events to be notified about changes to a db instance, db snapshot, or db parameter group. for more information, see monitoring metrics in an amazon rds instance. ^^how to work with amazon rds^^ there are several ways that you can interact with amazon rds. ^aws management console^ the aws management console is a simple web-based user interface. you can manage your db instances from the console with no programming required. to access the amazon rds console, sign in to the aws management console and open the amazon rds console at https://console.aws.amazon.com/rds/ ^command line interface^',\n",
       " 'you can use the aws command line interface (aws cli) to access the amazon rds api interactively. to install the aws cli, see installing the aws command line interface. to begin using the aws cli for rds, see aws command line interface reference for amazon rds. ^amazon rds apis^ if you are a developer, you can access the amazon rds programmatically using apis. for more information, see amazon rds api reference. for application development, we recommend that you use one of the aws software development kits (sdks) the aws sdks handle low-level details such as authentication, retry logic, and error handling, so that you can focus on your application logic. aws sdks are available for a wide variety of languages. for more information, see tools for amazon web services. aws also provides libraries, sample code, tutorials, and other resources to help you get started more easily. for more information, see sample code & libraries.',\n",
       " \"^^how you are charged for amazon rds^^ when you use amazon rds, you can choose to use on-demand db instances or reserved db instances. for more information, see db instance billing for amazon rds. for amazon rds pricing information, see the amazon rds product page. ^^what's next?^^ the preceding section introduced you to the basic infrastructure components that rds offers. what should you do next? ^getting started^ create a db instance using instructions in getting started with amazon rds. ^topics specific to database engines^ you can review information specific to a particular db engine in the following sections: amazon rds for mariadb. amazon rds for microsoft sql server. amazon rds for mysql. amazon rds for oracle. amazon rds for postgresql. ^^amazon rds shared responsibility model^^ amazon rds is responsible for hosting the software components and infrastructure of db instances and db cluster.\",\n",
       " 'you are responsible for query tuning, which is the process of adjusting sql queries to improve performance. query performance is highly dependent on database design, data size, data distribution, application workload, and query patterns, which can vary greatly. monitoring and tuning are highly individualized processes that you own for your rds databases. you can use amazon rds performance insights and other tools to identify problematic queries.',\n",
       " \"^^^what is amazon aurora?^^^ amazon aurora (aurora) is a fully managed relational database engine that's compatible with mysql and postgresql. you already know how mysql and postgresql combine the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. the code, tools, and applications you use today with your existing mysql and postgresql databases can be used with aurora. with some workloads, aurora can deliver up to five times the throughput of mysql and up to three times the throughput of postgresql without requiring changes to most of your existing applications. aurora includes a high-performance storage subsystem. its mysql- and postgresql-compatible database engines are customized to take advantage of that fast distributed storage. the underlying storage grows automatically as needed. an aurora cluster volume can grow to a maximum size of 128 tebibytes (tib)\",\n",
       " 'aurora also automates and standardizes database clustering and replication, which are typically among the most challenging aspects of database configuration and administration. aurora is part of the managed database service amazon relational database service (amazon rds) amazon rds is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. if you are not already familiar with amazon rds, see the amazon relational database service user guide. to learn more about the variety of database options available on amazon web services, see choosing the right database for your organization on aws. ^^amazon rds shared responsibility model^^ amazon rds is responsible for hosting the software components and infrastructure of db instances and db clusters. you are responsible for query tuning, which is the process of adjusting sql queries to improve performance. query performance is highly dependent on database design, data size, data distribution, application workload, and query patterns, which can vary greatly.',\n",
       " 'monitoring and tuning are highly individualized processes that you own for your rds databases. you can use amazon rds performance insights and other tools to identify problematic queries. ^^how amazon aurora works with amazon rds^^ the following points illustrate how amazon aurora relates to the standard mysql and postgresql engines available in amazon rds: you choose aurora mysql or aurora postgresql as the db engine option when setting up new database servers through amazon rds. aurora takes advantage of the familiar amazon relational database service (amazon rds) features for management and administration. aurora uses the amazon rds aws management console interface, aws cli commands, and api operations to handle routine database tasks such as provisioning, patching, backup, recovery, failure detection, and repair. aurora management operations typically involve entire clusters of database servers that are synchronized through replication, instead of individual database instances.',\n",
       " 'the automatic clustering, replication, and storage allocation make it simple and cost-effective to set up, operate, and scale your largest mysql and postgresql deployments. you can bring data from amazon rds for mysql and amazon rds for postgresql into aurora by creating and restoring snapshots, or by setting up one-way replication. you can use push-button migration tools to convert your existing rds for mysql and rds for postgresql applications to aurora. before using amazon aurora, complete the steps in setting up your environment for amazon aurora, and then review the concepts and features of aurora in amazon aurora db clusters.',\n",
       " '^^^architecture best practices for databases^^^ ^^featured topic: database modernization^^ ^guide^ ^pattern^ ^^choose a dr capability for amazon rds for…^^ ^^assess query performance for migrating sql…^^ html | pdf. tags: databases | storage & backup. html | pdf. tags: databases | migration. ^this is my architecture^ ^guide^ ^^3m: parallel serverless workflows for materials…^^ ^^best practices for deploying sql server…^^ learn how 3m uses a parallel serverless architecture to securely serve materials science information to a global team of scientists and business analysts. html | pdf. tags: databases. check out more resources for architecting in the #aws cloud: ^this is my architecture^ ^pattern^ ^^migrate oracle e-business suite to amazon rds…^^ ^^the new york times: giving developers the…^^',\n",
       " 'developers not only need the freedom to rapidly deploy and scale the cloud resources they need to build their applications, but also maintain organizational governance and control. in this episode, ahmed bebars from the new york times describes how they used aws controltower to manage, control and govern their aws accounts as well as providing isolation layers between their different services reducing deployment time from days to minutes and ensuring standards are maintained. html | pdf. tags: databases | infrastructure | migration. ^database blog posts^ ^learn about aws^ ^resources for aws^ ^developers on aws^ ^help^',\n",
       " '^^^what is aws lambda?^^^ aws lambda is a compute service that lets you run code without provisioning or managing servers. lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, and logging. with lambda, all you need to do is supply your code in one of the language runtimes that lambda supports. you organize your code into lambda functions. the lambda service runs your function only when needed and scales automatically. you only pay for the compute time that you consume—there is no charge when your code is not running. for more information, see aws lambda pricing. to learn how to build serverless solutions, check out the serverless developer guide. ^^when to use lambda^^ lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to zero when not in demand. for example, you can use lambda for:',\n",
       " 'file processing: use amazon simple storage service (amazon s3) to trigger lambda data processing in real time after an upload. stream processing: use lambda and amazon kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, internet of things (iot) device data telemetry, and metering. web applications: combine lambda with other aws services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers. iot backends: build serverless backends using lambda to handle web, mobile, iot, and third-party api requests. mobile backends: build backends using lambda and amazon api gateway to authenticate and process api requests. use aws amplify to easily integrate with your ios, android, web, and react native frontends. when using lambda, you are responsible only for your code.',\n",
       " 'lambda manages the compute fleet that offers a balance of memory, cpu, network, and other resources to run your code. because lambda manages these resources, you cannot log in to compute instances or customize the operating system on provided runtimes. lambda performs operational and administrative activities on your behalf, including managing capacity, monitoring, and logging your lambda functions. if you do need to manage your compute resources, aws has other compute services to consider, such as: aws app runner builds and deploys containerized web applications automatically, load balances traffic with encryption, scales to meet your traffic needs, and allows for the configuration of how services are accessed and communicate with other aws applications in a private amazon vpc. aws fargate with amazon ecs runs containers without having to provision, configure, or scale clusters of virtual machines. amazon ec2 lets you customize operating system, network and security settings, and the entire software stack.',\n",
       " \"you are responsible for provisioning capacity, monitoring fleet health and performance, and using availability zones for fault tolerance. ^^key features^^ the following key features help you develop lambda applications that are scalable, secure, and easily extensible: configure your lambda function using the console or aws cli. use environment variables to adjust your function's behavior without updating code. manage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version. create a container image for a lambda function by using an aws provided base image or an alternative base image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning. package libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code. augment your lambda functions with tools for monitoring, observability, security, and governance. add a dedicated http(s) endpoint to your lambda function.\",\n",
       " 'configure your lambda function urls to stream response payloads back to clients from node.js functions, to improve time to first byte (ttfb) performance or to return larger payloads. apply fine-grained control over the scaling and responsiveness of your production applications. verify that only approved developers publish unaltered, trusted code in your lambda functions. create a private network for resources such as databases, cache instances, or internal services. configure a function to mount an amazon elastic file system (amazon efs) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency. improve startup performance for java runtimes by up to 10x at no extra cost, typically with no changes to your function code.',\n",
       " \"^^^api reference^^^ this section contains the aws lambda api reference documentation. when making the api calls, you will need to authenticate your request by providing a signature. aws lambda supports signature version 4 for more information, see signature version 4 signing process in the amazon web services general reference. for an overview of the service, see what is aws lambda? you can use the aws cli to explore the aws lambda api. this guide provides several tutorials that use the aws cli. topics. actions. data types. ^^certificate errors when using an sdk^^ because aws sdks use the ca certificates from your computer, changes to the certificates on the aws servers can cause connection failures when you attempt to use an sdk. you can prevent these failures by keeping your computer's ca certificates and operating system up-to-date. if you encounter this issue in a corporate environment and do not manage your own computer, you might need to ask an administrator to assist with the update process.\",\n",
       " 'the following list shows minimum operating system and java versions: microsoft windows versions that have updates from january 2005 or later installed contain at least one of the required cas in their trust list. mac os x 10.4 with java for mac os x 10.4 release 5 (february 2007), mac os x 10.5 (october 2007), and later versions contain at least one of the required cas in their trust list. red hat enterprise linux 5 (march 2007), 6, and 7 and centos 5, 6, and 7 all contain at least one of the required cas in their default trusted ca list.',\n",
       " 'java 1.4.2_12 (may 2006), 5 update 2 (march 2005), and all later versions, including java 6 (december 2006), 7, and 8, contain at least one of the required cas in their default trusted ca list. when accessing the aws lambda management console or aws lambda api endpoints, whether through browsers or programmatically, you will need to ensure your client machines support any of the following cas: amazon root ca 1 starfield services root certificate authority - g2 starfield class 2 certification authority. root certificates from the first two authorities are available from amazon trust services, but keeping your computer up-to-date is the more straightforward solution. to learn more about acm-provided certificates, see aws certificate manager faqs.',\n",
       " '^^^introduction^^^ aws lambda is a flexible service designed for a wide variety of use-cases. across the millions of aws customers using lambda every month, serverless applications generally fall into several common categories: web applications: serve the front-end code via amazon s3 and amazon cloudfront, or automating the entire deployment and hosting with aws amplify console. web and mobile backends: the front-ends interact with the backend via api gateway. integrated authorization and authentication are provided by amazon cognito or apn partners like auth0 data processing: event-based processing tasks triggered by data changes in data stores, or streaming data etl tasks with amazon kinesis and lambda. parallelized computing tasks: splitting highly complex, long-lived computations to individual tasks across many lambda function instances to process data more quickly in parallel. internet of things (iot) workloads: processing data generated by physical iot devices.',\n",
       " 'additionally, many workloads are hybrid serverless applications, especially where legacy systems are being migrated from either on-premises or instance-based environments. in this case, developers can gradually migrate functionality from a legacy system to a lambda-based application. this guide is built for developers and operators of lambda-based applications. it is aimed at operators of typical production serverless applications, looking to understand more clearly how to build, measure, troubleshoot, and optimize their compute processes. this guide covers concepts and best practices for designing lambda-based applications, together with an approach for ongoing monitoring and troubleshooting. serverless applications can include a wide variety of different aws services to manage apis, messaging, storage, and content distribution. most of these applications rely upon lambda for connecting these services and transforming the data throughout an application. this guide focuses on the role of lambda in these architectures, and how you can fine-tune your functions and their configurations to maximize reliability and maintainability, and reduce cost.',\n",
       " 'lambda-based applications are event-driven architectures with many of the characteristics of distributed systems. while lambda handles many of the complex tasks like scaling and infrastructure management, it’s important for operators to understand the scope of knowledge that they need to manage serverless applications successfully. as aws customers adopt lambda to solve many of their most challenging workloads, understanding the troubleshooting and monitoring tasks involved is the key to becoming a proficient operator. both start-ups and enterprises develop lambda-based applications for green field applications and legacy applications. as these applications develop features and build traffic, many of the same best practices for operations apply. this guide covers many of the most important operational best practices and advice while explaining core topics underpinning how lambda-based applications work. the first half of this guide provides a deep dive into foundational topics around event-driven architectures, application design, and security. the latter half covers guidance for operators around debugging, monitoring and observability, and performance optimization.',\n",
       " 'the goal is to provide a concrete, actionable approach to operating and troubleshooting lambda-based applications. these are the topics covered in detail: event-driven architectures: understanding how events drive serverless applications informs the design of your workload. this chapter explains: how lambda fits into this paradigm; the benefits and tradeoffs of event-driven architectures; design principles, stateless design, idempotency, and message ordering; retry behaviors; using aws services; avoiding common anti-patterns. application design: topics include: understanding quotas; scaling and concurrency; choosing and managing runtimes and sdks; networking and vpc configurations; comparing synchronous versus asynchronous invocations; controlling traffic flow for non-serverless services. security: security is the primary concern at aws, but all developers have a role to play in developing secure applications. this covers: the shared responsibility model; applying the principles of least privilege; handling sensitive data; iam roles and resource policies; authorization and authentication; code signing; protecting applications with public endpoints.',\n",
       " 'debugging: the process of identifying errors in software is critical to any production workload. topics include: standardizing a debugging approach; capturing and replaying events; troubleshooting executions, networking, and deployments; identifying common causes of errors (memory configurations, timeouts, quotas, third-party libraries, and unintended leakage between invocations) monitoring and observability: serverless applications have parallels with distributed applications for monitoring and observability, presenting challenges to new serverless operators. this chapter explains: instrumentation best practices; cloudwatch logs (using insights and aws resource groups); tracing with x-ray; alerts and automation; code storage optimization. performance optimization: while lambda manages running and scaling functions, there are many levers available to developers that influence the performance. topics include: cold starts and latency, package sizes and dependencies, memory and power settings; performance and cost; maximizing throughput. this guide will be revised regularly to incorporate new lambda features and aws services as they are released.if you have any questions or comments about any of the content in this guide, raise an issue in the github repository or contact the author. james beswick aws serverless developer advocate jbeswick@amazon.com.',\n",
       " \"^^^what is amazon vpc?^^^ with amazon virtual private cloud (amazon vpc), you can launch aws resources in a logically isolated virtual network that you've defined. this virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of aws. the following diagram shows an example vpc. the vpc has one subnet in each of the availability zones in the region, ec2 instances in each subnet, and an internet gateway to allow communication between the resources in your vpc and the internet. for more information, see amazon virtual private cloud (amazon vpc) ^^features^^ the following features help you configure a vpc to provide the connectivity that your applications need: a vpc is a virtual network that closely resembles a traditional network that you'd operate in your own data center. after you create a vpc, you can add subnets. a subnet is a range of ip addresses in your vpc.\",\n",
       " 'a subnet must reside in a single availability zone. after you add subnets, you can deploy aws resources in your vpc. you can assign ip addresses, both ipv4 and ipv6, to your vpcs and subnets. you can also bring your public ipv4 and ipv6 gua addresses to aws and allocate them to resources in your vpc, such as ec2 instances, nat gateways, and network load balancers. use route tables to determine where network traffic from your subnet or gateway is directed. a gateway connects your vpc to another network. for example, use an internet gateway to connect your vpc to the internet. use a vpc endpoint to connect to aws services privately, without the use of an internet gateway or nat device. use a vpc peering connection to route traffic between the resources in two vpcs. copy network traffic from network interfaces and send it to security and monitoring appliances for deep packet inspection.',\n",
       " 'use a transit gateway, which acts as a central hub, to route traffic between your vpcs, vpn connections, and aws direct connect connections. a flow log captures information about the ip traffic going to and from network interfaces in your vpc. connect your vpcs to your on-premises networks using aws virtual private network (aws vpn) ^^getting started with amazon vpc^^ your aws account includes a default vpc in each aws region. your default vpcs are configured such that you can immediately start launching and connecting to ec2 instances. for more information, see get started with amazon vpc. you can choose to create additional vpcs with the subnets, ip addresses, gateways and routing that you need. for more information, see create a vpc. ^^working with amazon vpc^^ you can create and manage your vpcs using any of the following interfaces:',\n",
       " \"aws management console — provides a web interface that you can use to access your vpcs. aws command line interface (aws cli) — provides commands for a broad set of aws services, including amazon vpc, and is supported on windows, mac, and linux. for more information, see aws command line interface. aws sdks — provides language-specific apis and takes care of many of the connection details, such as calculating signatures, handling request retries, and error handling. for more information, see aws sdks. query api — provides low-level api actions that you call using https requests. using the query api is the most direct way to access amazon vpc, but it requires that your application handle low-level details such as generating the hash to sign the request, and error handling. for more information, see amazon vpc actions in the amazon ec2 api reference. ^^pricing for amazon vpc^^ there's no additional charge for using a vpc.\",\n",
       " 'there are charges for some vpc components, such as nat gateways, ip address manager, traffic mirroring, reachability analyzer, and network access analyzer. for more information, see amazon vpc pricing. pricing for public ipv4 addresses. public ipv4 addresses are charged. for specific pricing information, see the public ipv4 address tab in amazon vpc pricing. private ipv4 addresses (rfc 1918) are not charged. nearly all resources you launch in your vpc come with an ip address for connectivity. while the vast majority of resources in your vpc use private ipv4 addresses, resources that require direct access to the internet over ipv4 use public ipv4 addresses. a public ipv4 address is an ipv4 address that is routable from the internet. a public ipv4 address is necessary for a resource to be directly reachable from the internet over ipv4 the types of public ipv4 addresses are:',\n",
       " 'elastic ip addresses (eips): static, public ipv4 addresses provided by amazon that you can associate with an ec2 instance, elastic network interface, or aws resource. ec2 public ipv4 addresses: public ipv4 addresses assigned to an ec2 instance by amazon (if the ec2 instance is launched into a default subnet or if the instance is launched into a subnet that’s been configured to automatically assign a public ipv4 address) byoipv4 addresses: public ipv4 addresses in the ipv4 address range that you’ve brought to aws using bring your own ip addresses (byoip) service-managed ipv4 addresses: public ipv4 addresses automatically provisioned on aws resources and managed by an aws service. for example, public ipv4 addresses on amazon ecs, amazon rds, or amazon workspaces. below is a list of most common aws services that can use public ipv4 addresses. amazon appstream 2.0 aws database migration service. amazon ec2',\n",
       " 'amazon elastic container service. amazon eks. amazon emr. aws global accelerator. aws mainframe modernization. amazon managed streaming for apache kafka. amazon mq. amazon rds. amazon redshift. aws site-to-site vpn. amazon vpc nat gateway. amazon workspaces. elastic load balancing.',\n",
       " '^^^what is vpc peering?^^^ a virtual private cloud (vpc) is a virtual network dedicated to your aws account. it is logically isolated from other virtual networks in the aws cloud. you can launch aws resources, such as amazon ec2 instances, into your vpc. a vpc peering connection is a networking connection between two vpcs that enables you to route traffic between them using private ipv4 addresses or ipv6 addresses. instances in either vpc can communicate with each other as if they are within the same network. you can create a vpc peering connection between your own vpcs, or with a vpc in another aws account. the vpcs can be in different regions (also known as an inter-region vpc peering connection)',\n",
       " 'aws uses the existing infrastructure of a vpc to create a vpc peering connection; it is neither a gateway nor a vpn connection, and does not rely on a separate piece of physical hardware. there is no single point of failure for communication or a bandwidth bottleneck. a vpc peering connection helps you to facilitate the transfer of data. for example, if you have more than one aws account, you can peer the vpcs across those accounts to create a file sharing network. you can also use a vpc peering connection to allow other vpcs to access resources you have in one of your vpcs. when you establish peering relationships between vpcs across different aws regions, resources in the vpcs (for example, ec2 instances and lambda functions) in different aws regions can communicate with each other using private ip addresses, without using a gateway, vpn connection, or network appliance. the traffic remains in the private ip space.',\n",
       " 'all inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck. traffic always stays on the global aws backbone, and never traverses the public internet, which reduces threats, such as common exploits, and ddos attacks. inter-region vpc peering provides a simple and cost-effective way to share resources between regions or replicate data for geographic redundancy. ^^pricing for a vpc peering connection^^ there is no charge to create a vpc peering connection. all data transfer over a vpc peering connection that stays within an availability zone (az) is free. charges apply for data transfer over a vpc peering connections that cross availability zones and regions. for more information, see amazon ec2 pricing.',\n",
       " '^^^what is traffic mirroring?^^^ traffic mirroring is an amazon vpc feature that you can use to copy network traffic from an elastic network interface of type interface. you can then send the traffic to out-of-band security and monitoring appliances for: content inspection. threat monitoring. troubleshooting. the security and monitoring appliances can be deployed as individual instances, or as a fleet of instances behind either a network load balancer or a gateway load balancer with a udp listener. traffic mirroring supports filters and packet truncation, so that you can extract only the traffic of interest, using the monitoring tools of your choice. ^^traffic mirroring concepts^^ the following are the key concepts for traffic mirroring: source — the network interface to monitor. filter — a set of rules that defines the traffic that is mirrored. target — the destination for mirrored traffic. session — establishes a relationship between a source, a filter, and a target. ^^work with traffic mirroring^^',\n",
       " 'you can create, access, and manage your traffic mirror resources using any of the following: aws management console— provides a web interface that you can use to access your traffic mirror resources. aws command line interface (aws cli) — provides commands for a broad set of aws services, including amazon vpc. the aws cli is supported on windows, macos, and linux. for more information, see aws command line interface. aws sdks — provide language-specific apis. the aws sdks take care of many of the connection details, such as calculating signatures, handling request retries, and handling errors. for more information, see aws sdks. query api— provides low-level api actions that you call using https requests. using the query api is the most direct way to access amazon vpc. however, it requires that your application handle low-level details such as generating the hash to sign the request and handling errors. for more information, see amazon vpc actions in the amazon ec2 api reference.',\n",
       " \"^^traffic mirroring benefits^^ traffic mirroring offers the following benefits: simplified operation — mirror any range of your vpc traffic without having to manage packet forwarding agents on your ec2 instances. enhanced security — capture packets at the elastic network interface, which cannot be disabled or tampered with from a user space. increased monitoring options — send your mirrored traffic to any security device. ^^pricing^^ you are charged on an hourly basis for each active traffic mirror session. you'll continue to be charged for traffic mirroring until you delete all active traffic mirror sessions. for example, you'll still be charged in the following scenarios: you detached the network interface from the mirror source. you stopped or terminated the mirror source. you changed the instance type of the mirror source to an unsupported instance type. for the steps to delete a traffic mirror session, see delete a traffic mirror session. for information about pricing for traffic mirroring, see network analysis on the amazon vpc pricing page.\",\n",
       " '^^^amazon vpc actions^^^ the following api actions are available for amazon vpc. to learn more about amazon vpc, see the amazon vpc product page and the amazon vpc documentation. associatedhcpoptions. createdhcpoptions. deletedhcpoptions. describedhcpoptions. assignipv6addresses. assignprivateipaddresses. attachnetworkinterface. createnetworkinterface. createnetworkinterfacepermission. deletenetworkinterface. deletenetworkinterfacepermission. describenetworkinterfaceattribute. describenetworkinterfacepermissions. describenetworkinterfaces. detachnetworkinterface. modifynetworkinterfaceattribute. resetnetworkinterfaceattribute. unassignipv6addresses. unassignprivateipaddresses. attachinternetgateway. createegressonlyinternetgateway. createinternetgateway. deleteegressonlyinternetgateway. deleteinternetgateway. describeegressonlyinternetgateways. describeinternetgateways. detachinternetgateway. createmanagedprefixlist. deletemanagedprefixlist. describemanagedprefixlists. describeprefixlists.',\n",
       " 'getmanagedprefixlistassociations. getmanagedprefixlistentries. modifymanagedprefixlist. restoremanagedprefixlistversion. createnatgateway. deletenatgateway. describenatgateways. createnetworkacl. createnetworkaclentry. deletenetworkacl. deletenetworkaclentry. describenetworkacls. replacenetworkaclassociation. replacenetworkaclentry. associateroutetable. createroute. createroutetable. deleteroute. deleteroutetable. describeroutetables. disassociateroutetable. replaceroute. replaceroutetableassociation. authorizesecuritygroupegress. authorizesecuritygroupingress. createsecuritygroup. deletesecuritygroup. describesecuritygroupreferences. describesecuritygroups. describestalesecuritygroups. modifysecuritygrouprules. revokesecuritygroupegress. revokesecuritygroupingress. updatesecuritygroupruledescriptionsegress. updatesecuritygroupruledescriptionsingress. associatesubnetcidrblock. createdefaultsubnet. createsubnet.',\n",
       " 'createsubnetcidrreservation. deletesubnet. deletesubnetcidrreservation. describesubnets. disassociatesubnetcidrblock. getsubnetcidrreservations. modifysubnetattribute. createtrafficmirrorfilter. createtrafficmirrorfilterrule. createtrafficmirrorsession. createtrafficmirrortarget. deletetrafficmirrorfilter. deletetrafficmirrorfilterrule. deletetrafficmirrorsession. deletetrafficmirrortarget. describetrafficmirrorfilters. describetrafficmirrorsessions. describetrafficmirrortargets. modifytrafficmirrorfilternetworkservices. modifytrafficmirrorfilterrule. modifytrafficmirrorsession. associatevpccidrblock. createdefaultvpc. createvpc. deletevpc. describevpcattribute. describevpcs. disassociatevpccidrblock. modifyvpcattribute. modifyvpctenancy. createflowlogs. deleteflowlogs. describeflowlogs. getflowlogsintegrationtemplate. acceptvpcpeeringconnection.createvpcpeeringconnection. deletevpcpeeringconnection. describevpcpeeringconnections. modifyvpcpeeringconnectionoptions. rejectvpcpeeringconnection.',\n",
       " '^^^choosing an aws machine learning service^^^ ^pick the right ai and ml services, frameworks, and foundation models to support your work^ ^^introduction^^ 25 minutes. at its most basic, machine learning (ml) is designed to provide digital tools and services to learn from data, identify patterns, make predictions, and then act on those predictions. almost all artificial intelligence (ai) systems today are created using ml. ml uses large amounts of data to create and validate decision logic. this decision logic forms the basis of the ai \"model\".\\n\\na fast-growing subset of machine learning is generative ai, which is powered by large models that are pretrained on a vast set of data - commonly referred to as foundation models (fms) aws services based on generative ai include: help determine which aws ml services are the best fit for your needs. beginner. july 26, 2023',\n",
       " 'amazon bedrock\\namazon codewhisperer\\namazon comprehend\\namazon textract\\namazon translate\\namazon lex\\namazon polly\\namazon transcribe\\namazon rekognition\\namazon sagemaker. this decision guide will help you ask the right questions, evaluate your criteria, and determine which services are the best fit for your needs. analytics\\napplication integration\\ncontainers\\ndatabases\\ndevelopment strategy\\nstorage. in less than two minutes, dr. werner vogels, amazon cto explains how generative ai works and how you might use it. this video is part of a longer discussion between dr. vogels and swami sivasubramanian, aws vice-president of database, analytics, and ml, about the broad landscape of generative ai, why it’s not hype, and how aws is democratizing access to large language and foundation models. ^^understand^^ ^^consider^^ when solving a business problem with aws ml services, consideration of several key criteria can help ensure success.',\n",
       " 'the following section outlines some of the key criteria to consider when choosing a ml service. the first step in the ml lifecycle is to frame the business problem. understanding the problem you are trying to solve is essential for choosing the right aws ml service, as different services are designed to address different problems. it is also important to determine whether ml is the best fit for your business problem. once you have determined that ml is the best fit, you can start by choosing choosing from among a range of purpose-built aws ai services (in areas such as speech, vision and documents) amazon sagemaker provides fully managed infrastructure if you need to build and train your own models. aws offers an array of advanced ml frameworks and infrastructure choices for the cases where you require highly customized and specialized ml models. aws also offers a broad set of popular foundation models for building new applications with generative ai. ^^choose^^',\n",
       " 'now that you know the criteria by which you will be evaluating your ml service options, you are ready to choose which aws ml service is right for your organizational needs.\\n\\nthe following table highlights which ml services are optimized for which circumstances. use it to help determine the aws ml service that is the best fit for your use case. ^^use^^ now that you have learned about the criteria you need to apply in choosing an aws ml service, we hope you have been able to select which aws ai/ml service(s) are optimized for your organizational needs. to explore how to use and learn more about your chosen service, we have provided three sets of pathways to explore how each service works. the first set of pathways provides in-depth documentation, hands-on tutorials, and resources to get started with amazon comprehend, amazon textract, amazon translate, amazon lex, amazon polly, amazon rekognition, and amazon transcribe. get started with amazon comprehend.',\n",
       " 'analyze insights in text with amazon comprehend. amazon comprehend pricing\\n\\nthis short guide provides information on amazon comprehend pricing, along with examples. this exercise uses the amazon comprehend console to create and run an asynchronous entity detection job. it assumes that you are familiar with amazon simple storage service (amazon s3).\\n\\ndo the exercise » in this step-by-step tutorial, you learn how to use amazon comprehend to analyze and derive insights from text. in this scenario, you’re planning a trip and want to find helpful travel books. get pricing guidance » use the tutorial » the second set of ai/ml aws service pathways provide in-depth documentation, hands-on tutorials, and resources to get started with the services in the amazon sagemaker family. how amazon sagemaker works\\n\\nthis guide provides an overview of machine learning and explains how sagemaker works. getting started with amazon sagemaker\\n\\nuse this guide to setup sagemaker.',\n",
       " 'it will show you how to join an amazon sagemaker domain, giving you access to amazon sagemaker studio and rstudio on sagemaker. read the guide >> use apache spark with amazon sagemaker\\n\\nthis guide is for developers who want to use apache spark for preprocessing data and sagemaker for model training and hosting. read the guide >> read the guide >> use docker containers to build models\\n\\namazon sagemaker makes extensive use of docker containers for build and runtime tasks. it provides pre-built docker images for its built-in algorithms and the supported deep learning frameworks used for training and inference. this guide show how to deploy these containers. read the guide >> machine learning frameworks and languages\\n\\nyou can use python and r natively in amazon sagemaker notebook kernels. there are also kernels that support specific frameworks. this guide explores how to get started with sagemaker using the amazon sagemaker python sdk. read the guide >>',\n",
       " \"the third set of ai/ml aws service pathways provide in-depth documentation, hands-on tutorials, and resources to get started with amazon bedrock, amazon codewhisperer, aws trainium, aws inferentia, and amazon titan. overview of amazon bedrock. announcing new tools for building with generative ai on aws. demystifying generative ai. in this video, dr. werner vogels, amazon cto and swami sivasubramanian, aws vp of database, analytics, and ml, sit down to discuss the broad landscape of generative ai, why it’s not hype, and how aws is democratizing access to large language and foundation models. amazon bedrock is a fully managed service that makes foundation models from leading ai startups and amazon available via an api, so you can choose from a wide range of fms to find the model that's best suited for your use case. the overview provides details how and when it can be used.\",\n",
       " 'this blog post provides background on the development of amazon bedrock, how it fits with the broader aws approach to ai and ml - and provides an overview of potential uses for aws generative ai services. read the blog » watch the video » read the overview » ^^explore^^ explore whitepapers to help you get started and learn best practices in choosing and using ai/ml services. explore vetted solutions and architectural guidance for common use cases for ai and ml services. these reference architecture diagrams show examples of aws ai and ml services in use. explore architecture diagrams » explore solutions » explore whitepapers » ^learn about aws^ ^resources for aws^ ^developers on aws^ ^help^',\n",
       " '^^^choosing an aws container service^^^ ^taking the first step^ 20 minutes. ^^introduction^^ containers are a key component of modern app development. they have become the standard way to organize compute resources, and manage the content of your application deployments.\\n\\ncontainers provide a discrete reproducible compute environment. they also provide a way to simplify packaging and dependency management. from the orchestration of very large multi-cluster estates to web applications - or even testing your work and doing a proof of concept on your laptop - they are a great way to get started and build software to deploy in the cloud. help determine which aws container service is the best fit for your organization. beginner. april 26, 2023 amazon ec2\\namazon ecr\\namazon ecs\\namazon eks\\namazon lightsail\\naws app runner\\naws elastic beanstalk\\naws fargate\\nred hat openshift service on aws.',\n",
       " 'this decision guide helps you get started and choose the right aws container service for your modern app development. analytics\\napplication integration\\ndatabases\\ndevelopment strategy\\nmachine learning\\nstorage. this four-minute excerpt is from a 57 minute recording of a presentation by vikram venkataraman, a principal technical account manager at aws, given at aws summit dc 2022 it provides an overview of available aws container services. ^^understand^^ containers have become a de facto standard for packaging application code, configurations, and dependencies into a single artifact, because they can be deployed in a consistent manner to multiple environments.\\n\\ncontainers are a key piece of any modern application development strategy. to provide some context for container-related aws services,  the image on the right shows the available aws container options by layer (explained more fully in this video) capacity. capacity refers to the underlying compute that you will deploy your container to, your central processing unit (cpu), and memory requirements.',\n",
       " \"capacity is the infrastructure the container is going to run on. there are two choices for capacity for your containers on aws: orchestration. it is not uncommon for organizations to be running not one or a few but thousands of containers. it becomes challenging to manage such a complex environment. orchestration services help facilitate the deployment and management of these applications. aws offers three orchestration services: provisioning. provisioning provides an interface to the orchestration layer of services. each orchestrator comes with its own complexities. provisioning services hide this complexity and helps make it easier for you to consume the service. aws services in this category include: ^^consider^^ it's important to choose a container service that aligns to your application requirements and operational preferences. the following section outlines some of the key criteria to consider when choosing a container service, as well as supporting tools and services. organizations may choose the cloud to reduce operational cost by standardizing on managed services that shift the operational burden to aws.\",\n",
       " 'higher levels of abstraction allow developers and operators to focus on their own unique value-add activities, instead of undifferentiated tasks.\\n\\nbuilding with containers on aws uses services with higher levels of abstraction to shift the operational overhead of maintaining infrastructure to aws. ^^choose^^ now that you know the criteria by which you will be evaluating your container options, you are ready to choose which aws container service(s) may be a good fit for your organizational requirements. the following table highlights which services are optimized for which circumstances. use the table to help determine which container service is the best fit for your organization and use case. ^^use^^ you should now have a clear understanding of each aws container service (and the supporting aws tools and services) and which one might be the best fit for your organization and use case. to explore how to use and learn more about each of the available aws container services, we have provided a pathway to explore how each of the services work.',\n",
       " 'the following section provides links to in-depth documentation, hands-on tutorials, and resources to get you started. getting started with amazon ec2\\n\\naccess the complete set of amazon ec2 technical documentation, including guides to linux and windows instances. vm import/export user guide\\n\\nlearn how to import virtual machine (vm) images from your existing virtualization environment to amazon ec2, and then export them back. amazon ec2 auto scaling with ec2 spot instances\\n\\nlearn how to create a stateless, fault tolerant workload using amazon ec2 auto scaling with launch templates to request amazon ec2 spot instances. explore the guides » explore the guide » get started with the tutorial » deploy a web application on amazon ec2 we walk you through creating an amazon ec2 instance using aws cdk, and deploying a web application on it. explore the guide » ^explore^ explore whitepapers to help you get started and learn best practices. explore vetted solutions and architectural guidance for common use cases for containers.explore reference architecture diagrams for containers on aws. explore architecture diagrams » explore whitepapers » explore solutions » ^learn about aws^ ^resources for aws^ ^developers on aws^ ^help^',\n",
       " '^^^choosing an aws database service^^^ ^taking the first step^ ^^introduction^^ 20 minutes. amazon web services (aws) offers a growing number of purpose-built database options (currently more than 15) to support diverse data models. these include relational, key-value, document, in-memory, graph, time series, wide column, and ledger databases. help determine which aws database(s) are the best fit for your organization. choosing the right database or multiple databases requires you to make a series of decisions based on your organizational needs. this decision guide will help you ask the right questions, provide a clear path for implementation, and help you migrate from your existing database. beginner. april 20, 2023 amazon aurora\\namazon documentdb\\namazon dynamodb\\namazon elasticache\\namazon keyspaces\\namazon memorydb\\namazon neptune\\namazon qldb\\namazon redshift\\namazon rds\\namazon timestream.',\n",
       " 'this six and a half minute video from aws developer advocate ricardo ferreira explains the basics of choosing an aws database, providing a strong introduction to the concepts, criteria and choices available to you in the rest of this decision guide. analytics\\napplication integration\\ncontainers\\ndevelopment strategy\\nmachine learning\\nstorage. ^^understand^^ databases are important backend systems used to store data for any type of app, whether it’s a small mobile app or an enterprise app with internet-scale and real-time requirements.\\n\\nthis decision guide is designed to help you understand the range of choices available to you, establish the criteria that make sense for you to make your database choice, provide you with detailed information on the unique properties of each database - and then allow you to dive deeper into the capabilities that each offers. what kinds of apps do people build using databases? note: this guide focuses on databases suitable for online transaction processing (oltp) applications.',\n",
       " \"if you primarily need to store and analyze massive amounts of data quickly and efficiently (typically met by an online analytical processing (olap) application), aws offers amazon redshift, a fully-managed, cloud-based data warehousing service that is designed to handle large-scale analytics workloads. there are two high-level categories of aws oltp databases - relational and non-relational. we'll explore all of these in detail in the choose section of this guide. database migration. before deciding which database service you want to use to work with your data, you may want to spend a little time thinking about how you're going to migrate your existing database(s) the best database migration strategy helps you take full advantage of the aws cloud. this involves migrating your applications to use purpose-built, cloud-centered databases. it also doesn't tie you to the same database that you've been using on premises. consider modernizing your applications and choose the databases that best suit your applications’ workflow requirements.\",\n",
       " \"the following resources can help you with your migration strategy: in addition to having a migration strategy at the front end of your planning, you want to have ways to gain insight from your data. you can use amazon redshift. it's a fast, fully managed, petabyte-scale data warehouse service that you can use to efficiently analyze all your data using your existing business intelligence tools. it's optimized for datasets that range from a few hundred gigabytes to a petabyte or more. ^^consider^^ you’re considering hosting a database on aws. this might be to support a greenfield/pilot project as a first step in your cloud migration journey, or you might want to migrate an existing workload with as little disruption as possible. or perhaps you would like to port your workload to managed aws services or even refactor it to fully cloud-native. whatever your goal, considering the right questions will make your database decision easier. here’s a summary of the key criteria to consider.\",\n",
       " 'the first major consideration when choosing your database is your business objective. what is the strategic direction driving your organization to change? as suggested in the 7 rs of aws, consider whether you want to re-architect or re-factor an existing workload, move to a new platform to shed commercial license commitments, rehost your existing databases and data to the cloud without making any changes to take advantage of cloud capabilities, or make the move now to a managed database strategy. your business objective will also drive the degrees of freedom you have in choosing a target database in aws for your workload. if you choose a rehost strategy, you might want to migrate the workload to aws with as few disruptions as possible. so you might adopt a “lift and shift” strategy where, for example, you migrate an on-premises oracle database to oracle on an ec2 instance. ^^choose^^',\n",
       " 'now that you know the criteria by which you will be evaluating your database options, you are ready to choose which aws database is right for your organizational needs. this table highlights which databases are optimized for which circumstances and type of data. use it to help determine the database that is the best fit for your use case. ^^use^^ now that you have learned about the shape of your data, how it fits in your environment, supports your use case, and what each database service is optimized for. you should have been able to select which aws database service(s) is optimized for your organizational needs. to explore how to use and learn more about each of the available aws database services - we have provided a pathway to explore how each of the services work. the following section provides links to in-depth documentation, hands-on tutorials, and resources to get you started. getting started with amazon aurora\\n\\nwe outline the basics of getting started with aurora.',\n",
       " \"this guide includes tutorials and covers more advanced aurora concepts and procedures, such as the different kinds of endpoints and how to scale aurora clusters up and down. create a high-availability database\\n\\nlearn how to configure an amazon aurora cluster to create a high-availability database. this database consists of compute nodes that are replicated across multiple availability zones to provide increased read scalability and failover protection. use amazon aurora global databases\\n\\nwe help you get started using aurora global databases. this guide outlines the supported engines and aws region availability for aurora global databases with aurora mysql and aurora postgresql. explore the guide » explore the guide » get started with the tutorial » migrate from amazon rds for mysql to amazon aurora mysql\\n \\nwe show you how to migrate any application's database from amazon rds for mysql to amazon aurora mysql with minimal downtime. this tutorial is not within the free tier and will cost you less than $1 create a serverless message processing application.\",\n",
       " 'we show you how to create a serverless message processing application with amazon aurora serverless (postgresql-compatible edition), data api for aurora serverless, aws lambda, and amazon sns. get started with the tutorial » get started with the tutorial » ^^explore^^ explore whitepapers to help you get started, learn best practices, and migrate your databases. explore vetted solutions and architectural guidance for common use cases for databases. explore reference architecture diagrams to help you develop, scale, and test your databases on aws. explore solutions » explore whitepapers » explore architecture diagrams » ^additional resources^ developers. amazon emr. solution architects. amazon kinesis. professional development. amazon msk. startups. amazon quicksight. amazon redshift. ^learn about aws^ ^resources for aws^ ^developers on aws^ ^help^']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset.content.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"content\"].to_csv(\"./data/mlm_dataset.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30522\n",
    "\n",
    "# Read input files from local input path \n",
    "paths = [str(x) for x in Path(\"./\").glob('*.txt')]\n",
    "\n",
    "# Train custom BertWordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(files=paths, vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# Save trained custom tokenizer to local output path\n",
    "tokenizer.save_model(\"./data/vocab\")\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('./data/vocab/vocab.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating custom tokenizer\n",
      "Test sentence: amazon simple storage service (amazon s3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n",
      "Encoded sentence: ['[CLS]', 'amazon', 'simple', 'storage', 'service', '(', 'amazon', 's3', ')', 'is', 'an', 'object', 'storage', 'service', 'that', 'offers', 'industry', '-', 'leading', 'scalability', ',', 'data', 'availability', ',', 'security', ',', 'and', 'performance', '.', '[SEP]']\n",
      "Token ID for token (s3) = 195\n",
      "Vocabulary size = 3029\n"
     ]
    }
   ],
   "source": [
    "# Evaluate custom tokenizer \n",
    "print('Evaluating custom tokenizer')\n",
    "test_sentence = 'amazon simple storage service (amazon s3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.'\n",
    "print(f'Test sentence: {test_sentence}')\n",
    "tokens = tokenizer.encode(test_sentence).tokens\n",
    "print(f'Encoded sentence: {tokens}')\n",
    "token_id = tokenizer.token_to_id('s3')\n",
    "print(f'Token ID for token (s3) = {token_id}')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f'Vocabulary size = {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'amazon',\n",
       " 'simple',\n",
       " 'storage',\n",
       " 'service',\n",
       " '(',\n",
       " 'amazon',\n",
       " 's3',\n",
       " ')',\n",
       " 'is',\n",
       " 'an',\n",
       " 'object',\n",
       " 'storage',\n",
       " 'service',\n",
       " 'that',\n",
       " 'offers',\n",
       " 'industry',\n",
       " '-',\n",
       " 'leading',\n",
       " 'scalability',\n",
       " ',',\n",
       " 'data',\n",
       " 'availability',\n",
       " ',',\n",
       " 'security',\n",
       " ',',\n",
       " 'and',\n",
       " 'performance',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(test_sentence).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Test\\\\vocab.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"./data/vocab\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Loka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
