{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ervin\\OneDrive\\Documents\\Loka\\Loka\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 21:07:54,788 - __main__ - INFO - [Using Transformers: 4.33.0.dev0]\n",
      "2023-08-31 21:07:54,789 - __main__ - INFO - [Using Datasets: 2.14.4]\n",
      "2023-08-31 21:07:54,790 - __main__ - INFO - [Using Torch: 2.0.1+cpu]\n",
      "2023-08-31 21:07:54,790 - __main__ - INFO - [Using Pandas: 2.1.0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertConfig\n",
    "from transformers import pipeline \n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import datasets\n",
    "import logging\n",
    "import torch\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.getLevelName('INFO'), \n",
    "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log versions of dependencies\n",
    "logger.info(f'[Using Transformers: {transformers.__version__}]')\n",
    "logger.info(f'[Using Datasets: {datasets.__version__}]')\n",
    "logger.info(f'[Using Torch: {torch.__version__}]')\n",
    "logger.info(f'[Using Pandas: {pd.__version__}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 21:11:19,919 - __main__ - INFO - Re-creating BERT tokenizer using custom vocabulary from [./vocab/]\n",
      "2023-08-31 21:11:19,934 - __main__ - INFO - Tokenizer: BertTokenizerFast(name_or_path='./vocab', vocab_size=3136, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 512\n",
    "# Re-create BERT WordPiece tokenizer \n",
    "logger.info(f'Re-creating BERT tokenizer using custom vocabulary from [./data/vocab/]')\n",
    "# config = BertConfig()\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./data/vocab\",padding=\"max_length\", truncation=\"max_length\")\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "tokenizer.init_kwargs['model_max_length'] = MAX_LENGTH\n",
    "tokenizer.save_pretrained(\"./BTF\")\n",
    "logger.info(f'Tokenizer: {tokenizer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:44:36,311 - __main__ - INFO - Re-creating BERT tokenizer using custom vocabulary from [./vocab/]\n",
      "2023-08-31 17:44:36,319 - __main__ - INFO - Tokenizer: BertTokenizerFast(name_or_path='./vocab', vocab_size=3136, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "2023-08-31 17:44:36,327 - __main__ - INFO - Chunked datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 177\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "})\n",
      "2023-08-31 17:44:36,328 - __main__ - INFO - Loading BertForMaskedLM model\n",
      "2023-08-31 17:44:37,851 - __main__ - INFO - Training MLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:44:40,941 - __main__ - INFO - Perplexity before training: 35577.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                              \n",
      " 20%|██        | 2/10 [01:54<06:49, 51.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.679203033447266, 'eval_runtime': 4.4669, 'eval_samples_per_second': 4.03, 'eval_steps_per_second': 0.672, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 40%|████      | 4/10 [04:08<05:48, 58.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.505746841430664, 'eval_runtime': 4.3143, 'eval_samples_per_second': 4.172, 'eval_steps_per_second': 0.695, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:58<03:39, 54.93s/it]\n",
      " 60%|██████    | 6/10 [06:02<03:39, 54.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.336270332336426, 'eval_runtime': 4.7532, 'eval_samples_per_second': 3.787, 'eval_steps_per_second': 0.631, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 80%|████████  | 8/10 [08:21<01:59, 59.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.20265007019043, 'eval_runtime': 4.8844, 'eval_samples_per_second': 3.685, 'eval_steps_per_second': 0.614, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 10/10 [10:13<00:00, 61.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.226667404174805, 'eval_runtime': 4.324, 'eval_samples_per_second': 4.163, 'eval_steps_per_second': 0.694, 'epoch': 5.0}\n",
      "{'train_runtime': 613.7378, 'train_samples_per_second': 1.442, 'train_steps_per_second': 0.016, 'train_loss': 9.502236938476562, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:54:59,078 - __main__ - INFO - Perplexity after training: 10688.52\n",
      "2023-08-31 17:54:59,084 - __main__ - INFO - Saving trained MLM to [./finetuned/]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_LENGTH = 512\n",
    "CHUNK_SIZE = 128\n",
    "TRAIN_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "SAVE_STEPS = 10000\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "\n",
    "LOCAL_DATA_DIR = './'\n",
    "LOCAL_MODEL_DIR = './finetuned'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Re-create original BERT WordPiece tokenizer \n",
    "# logger.info(f'Re-creating original BERT tokenizer')\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# logger.info(f'Tokenizer: {tokenizer}')\n",
    "\n",
    "# Re-create BERT WordPiece tokenizer \n",
    "logger.info(f'Re-creating BERT tokenizer using custom vocabulary from [./data/vocab/]')\n",
    "config = BertConfig()\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./data/vocab\", config=config, padding=\"max_length\", truncation=\"max_length\")\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "tokenizer.init_kwargs['model_max_length'] = MAX_LENGTH\n",
    "tokenizer.save_model(\"./BTF\")\n",
    "logger.info(f'Tokenizer: {tokenizer}')\n",
    "\n",
    "# Read dataset \n",
    "chunked_datasets = datasets.load_from_disk(LOCAL_DATA_DIR)\n",
    "logger.info(f'Chunked datasets: {chunked_datasets}')\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=True, \n",
    "                                                mlm_probability=0.15)\n",
    "    \n",
    "# Load MLM\n",
    "logger.info('Loading BertForMaskedLM model')\n",
    "mlm = BertForMaskedLM(config=config)\n",
    "\n",
    "# Train MLM\n",
    "logger.info('Training MLM')\n",
    "training_args = TrainingArguments(output_dir='/tmp/checkpoints', \n",
    "                                    overwrite_output_dir=True, \n",
    "                                    optim='adamw_torch',\n",
    "                                    num_train_epochs=TRAIN_EPOCHS,\n",
    "                                    per_device_train_batch_size=BATCH_SIZE,\n",
    "                                    evaluation_strategy='epoch',\n",
    "                                    save_steps=SAVE_STEPS, \n",
    "                                    save_total_limit=SAVE_TOTAL_LIMIT)\n",
    "trainer = Trainer(model=mlm, \n",
    "                    args=training_args, \n",
    "                    data_collator=data_collator,\n",
    "                    train_dataset=chunked_datasets['train'],\n",
    "                    eval_dataset=chunked_datasets['validation'])\n",
    "\n",
    "# Evaluate trained model for perplexity\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(f\"Perplexity before training: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(f\"Perplexity after training: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "# Save trained model to local model directory\n",
    "logger.info(f'Saving trained MLM to [{LOCAL_MODEL_DIR}/]')\n",
    "trainer.save_model(LOCAL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.276925086975098,\n",
       " 'eval_runtime': 4.18,\n",
       " 'eval_samples_per_second': 4.306,\n",
       " 'eval_steps_per_second': 0.718,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Loka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
