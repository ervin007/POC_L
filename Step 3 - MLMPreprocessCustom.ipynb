{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ervin\\OneDrive\\Documents\\Loka\\Loka\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import transformers \n",
    "import datasets\n",
    "import logging\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 15:05:20,894 - __main__ - INFO - [Using Transformers: 4.32.1]\n",
      "2023-08-31 15:05:20,896 - __main__ - INFO - [Using Datasets: 2.14.4]\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.getLevelName('INFO'), \n",
    "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log versions of dependencies\n",
    "logger.info(f'[Using Transformers: {transformers.__version__}]')\n",
    "logger.info(f'[Using Datasets: {datasets.__version__}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:44:08,260 - __main__ - INFO - Evaluating custom tokenizer\n",
      "2023-08-31 17:44:08,261 - __main__ - INFO - Test sentence: amazon simple storage service (amazon s3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n",
      "2023-08-31 17:44:08,262 - __main__ - INFO - Encoded sentence: [2, 137, 929, 338, 294, 10, 137, 203, 11, 180, 122, 254, 338, 294, 191, 932, 1871, 14, 2639, 2292, 13, 258, 602, 13, 587, 13, 131, 634, 15, 3]\n",
      "2023-08-31 17:44:08,263 - __main__ - INFO - Token ID for token (s3) = s3\n",
      "2023-08-31 17:44:08,264 - __main__ - INFO - Vocabulary size = 3136\n",
      "2023-08-31 17:44:08,264 - __main__ - INFO - Reading and collating input data to create mini batches for Masked Language Model (MLM) training\n",
      "2023-08-31 17:44:08,890 - __main__ - INFO - Dataset: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 223\n",
      "})\n",
      "2023-08-31 17:44:08,891 - __main__ - INFO - Splitting dataset into train and validation splits\n",
      "2023-08-31 17:44:08,894 - __main__ - INFO - Data splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 23\n",
      "    })\n",
      "})\n",
      "2023-08-31 17:44:08,895 - __main__ - INFO - Tokenizing dataset splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=12): 100%|██████████| 200/200 [00:04<00:00, 46.13 examples/s]\n",
      "Map (num_proc=12): 100%|██████████| 23/23 [00:04<00:00,  5.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:44:17,973 - __main__ - INFO - Tokenized datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 23\n",
      "    })\n",
      "})\n",
      "2023-08-31 17:44:17,974 - __main__ - INFO - Concatenating and chunking the datasets to a fixed length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map (num_proc=12): 100%|██████████| 200/200 [00:03<00:00, 56.86 examples/s]\n",
      "Map (num_proc=12): 100%|██████████| 23/23 [00:03<00:00,  6.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:44:25,464 - __main__ - INFO - Chunked datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 177\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "})\n",
      "2023-08-31 17:44:25,465 - __main__ - INFO - Saving chunked datasets to local disk ./\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 177/177 [00:00<00:00, 17697.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 18/18 [00:00<00:00, 2575.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:44:25,501 - __main__ - INFO - Validating if datasets were saved correctly\n",
      "2023-08-31 17:44:25,544 - __main__ - INFO - Reloaded dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 177\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Essentials\n",
    "# LOCAL_INPUT_PATH is mapped to S3 input location for covid news articles \n",
    "LOCAL_INPUT_PATH = './data/' \n",
    "# LOCAL_OUTPUT_PATH is mapped to S3 output location where we want to save the processed input data (COVID articles)\n",
    "LOCAL_OUTPUT_PATH = './'\n",
    "MAX_LENGTH = 128\n",
    "CHUNK_SIZE = 128\n",
    "N_GPUS = 0\n",
    "\n",
    "# Evaluate custom tokenizer \n",
    "logger.info('Evaluating custom tokenizer')\n",
    "test_sentence = 'amazon simple storage service (amazon s3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.'\n",
    "logger.info(f'Test sentence: {test_sentence}')\n",
    "tokens = tokenizer.encode(test_sentence)\n",
    "logger.info(f'Encoded sentence: {tokens}')\n",
    "token_id = tokenizer.convert_ids_to_tokens(203)\n",
    "logger.info(f'Token ID for token (s3) = {token_id}')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "logger.info(f'Vocabulary size = {vocab_size}')\n",
    "\n",
    "# Read dataset and collate to create mini batches for Masked Language Model (MLM) training\n",
    "logger.info('Reading and collating input data to create mini batches for Masked Language Model (MLM) training')\n",
    "dataset = load_dataset('text', data_files=f'{LOCAL_INPUT_PATH}/mlm_dataset.txt', split='train')\n",
    "logger.info(f'Dataset: {dataset}')\n",
    "\n",
    "# Split dataset into train and validation splits \n",
    "logger.info('Splitting dataset into train and validation splits')\n",
    "train_test_splits = dataset.train_test_split(shuffle=True, seed=123, test_size=0.1)\n",
    "data_splits = DatasetDict({'train': train_test_splits['train'], \n",
    "                           'validation': train_test_splits['test']})\n",
    "logger.info(f'Data splits: {data_splits}')\n",
    "    \n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(article):\n",
    "    # global tokenizer\n",
    "    from transformers import BertTokenizerFast\n",
    "    from transformers import BertConfig\n",
    "    config = BertConfig()\n",
    "    MAX_LENGTH = 128\n",
    "    CHUNK_SIZE = 128\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"./data/vocab\", config=config, padding=True, truncation=True)\n",
    "    tokenizer.model_max_length = MAX_LENGTH\n",
    "    tokenizer.init_kwargs['model_max_length'] = MAX_LENGTH\n",
    "    # print(\"WE ARE HREEEEE\")\n",
    "    tokenized_article = tokenizer(article['text'])\n",
    "    # print(\"WE APSSEDDDDD\")\n",
    "    if tokenizer.is_fast:\n",
    "        tokenized_article['word_ids'] = [tokenized_article.word_ids(i) for i in range(len(tokenized_article['input_ids']))]\n",
    "    return tokenized_article\n",
    "\n",
    "\n",
    "logger.info('Tokenizing dataset splits')\n",
    "num_proc = int(os.cpu_count()/1)\n",
    "# num_proc = 1\n",
    "# logger.info(f'Total number of processes = {num_proc}')\n",
    "tokenized_datasets = data_splits.map(tokenize, batched=True, num_proc=num_proc, remove_columns=['text'])\n",
    "logger.info(f'Tokenized datasets: {tokenized_datasets}')\n",
    "\n",
    "\n",
    "# Concat and chunk dataset \n",
    "def concat_and_chunk(articles):\n",
    "    CHUNK_SIZE = 128\n",
    "    # Concatenate all texts\n",
    "    # concatenated_examples = {key: sum(articles[key], []) for key in articles.keys()}\n",
    "    concatenated_examples = {key: [item for sublist in articles[key] for item in sublist] for key in articles.keys()}\n",
    "\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(articles.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length//CHUNK_SIZE) * CHUNK_SIZE\n",
    "    # Split by chunks of max_len\n",
    "    chunked_articles = {key: [text[i : i+CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)] for key, text in concatenated_examples.items()}\n",
    "    # Create a new labels column\n",
    "    chunked_articles['labels'] = chunked_articles['input_ids']\n",
    "    return chunked_articles\n",
    "    \n",
    "logger.info('Concatenating and chunking the datasets to a fixed length')\n",
    "chunked_datasets = tokenized_datasets.map(concat_and_chunk, batched=True, num_proc=num_proc)\n",
    "logger.info(f'Chunked datasets: {chunked_datasets}')\n",
    "\n",
    "# Save chunked datasets to local disk (EBS volume)\n",
    "logger.info(f'Saving chunked datasets to local disk {LOCAL_OUTPUT_PATH}')\n",
    "chunked_datasets.save_to_disk(f'{LOCAL_OUTPUT_PATH}')\n",
    "\n",
    "# Validate if datasets were saved correctly\n",
    "logger.info('Validating if datasets were saved correctly')\n",
    "reloaded_dataset = datasets.load_from_disk(f'{LOCAL_OUTPUT_PATH}')\n",
    "logger.info(f'Reloaded dataset: {reloaded_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Loka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
